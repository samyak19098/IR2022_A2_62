{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\Samyak\n",
      "[nltk_data]     Jain\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to C:\\Users\\Samyak\n",
      "[nltk_data]     Jain\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to C:\\Users\\Samyak\n",
      "[nltk_data]     Jain\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to C:\\Users\\Samyak\n",
      "[nltk_data]     Jain\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import copy\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '../data/Humor,Hist,Media,Food'\n",
    "file_names = os.listdir(data_dir) #reading the data directory to list all the files\n",
    "file_paths = [(data_dir + '/' + fname) for fname in file_names] #forming file paths\n",
    "docID_to_doc_mapping = {} #forming docID to doc name mapping\n",
    "for i in range(len(file_names)):\n",
    "    docID_to_doc_mapping[i] = file_names[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punct(tok):\n",
    "    '''\n",
    "        Removing punctations from tokens\n",
    "    '''\n",
    "    punctuations = string.punctuation\n",
    "    tok = ''.join(x for x in tok if x not in punctuations)\n",
    "    return tok\n",
    "def remove_space(tok):\n",
    "    '''\n",
    "        Removing blank space toks\n",
    "    '''\n",
    "    tok = ''.join(x for x in tok if x != ' ')\n",
    "    return tok\n",
    "\n",
    "def preprocess_file(file_text):\n",
    "    '''\n",
    "        This function preprocesses the file text.\n",
    "        Input: file_text in string form represting the text of a file\n",
    "        Returns: cleaned_toks, word tokens present in the file after preprocessing\n",
    "    '''\n",
    "\n",
    "    #converting the text to lowercase\n",
    "    ftext = file_text.lower()\n",
    "\n",
    "    #performing word tokenization\n",
    "    file_toks = word_tokenize(ftext)\n",
    "\n",
    "    #removing the stopwords from tokens\n",
    "    stop_words = list(set(stopwords.words('english')))\n",
    "    file_toks = [tok for tok in file_toks if tok not in stop_words]\n",
    "\n",
    "    #removing punctuation marks from tokens\n",
    "    toks_no_punct = []\n",
    "    for tok in file_toks:\n",
    "        ctok = remove_punct(tok)\n",
    "        if(ctok != \"\"):\n",
    "            toks_no_punct.append(ctok)\n",
    "    \n",
    "    #Removing blank space tokens\n",
    "    cleaned_toks = []\n",
    "    for tok in toks_no_punct:\n",
    "        ctok = remove_space(tok)\n",
    "        if(ctok != \"\"):\n",
    "            cleaned_toks.append(ctok)\n",
    "\n",
    "    return cleaned_toks\n",
    "\n",
    "def cleanQuery(query_text):\n",
    "    '''\n",
    "        Preprocessing the query text\n",
    "        Input: query_text, string of the phrase query text\n",
    "        Returns: cleaned_toks, an array containg the preprocessed query tokens\n",
    "    '''\n",
    "\n",
    "    #We perform the same preprocessing steps on the query as we did for the file text\n",
    "\n",
    "    #converting the text to lowercase\n",
    "    qtext = query_text.lower()\n",
    "    \n",
    "    #performing word tokenization\n",
    "    query_toks = word_tokenize(qtext)\n",
    "    \n",
    "    #removing the stopwords from tokens\n",
    "    stop_words = list(set(stopwords.words('english')))\n",
    "    query_toks = [tok for tok in query_toks if tok not in stop_words]\n",
    "    \n",
    "    #removing punctuation marks from tokens\n",
    "    toks_no_punct = []\n",
    "    for tok in query_toks:\n",
    "        ctok = remove_punct(tok)\n",
    "        if(ctok != \"\"):\n",
    "            toks_no_punct.append(ctok)\n",
    "    \n",
    "    #Removing blank space tokens\n",
    "    cleaned_toks = []\n",
    "    for tok in toks_no_punct:\n",
    "        ctok = remove_space(tok)\n",
    "        if(ctok != \"\"):\n",
    "            cleaned_toks.append(ctok)\n",
    "    \n",
    "    return cleaned_toks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(fpaths):\n",
    "    '''\n",
    "        Reads the files and preprocess every file's text to form word tokens for every file.\n",
    "        Returns a 2-D list containing word tokens for every file\n",
    "    '''\n",
    "    file_tokens = []\n",
    "    for fpath in fpaths:\n",
    "        f = open(fpath, 'r', encoding='utf-8', errors='ignore') #open the file\n",
    "        ftxt_unprocessed = f.read() #read the text of the file\n",
    "        ftoks = preprocess_file(ftxt_unprocessed) #preprocessing the text to form word tokens\n",
    "        file_tokens.append(ftoks)\n",
    "    return file_tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDocsFromID(docID_to_doc, doc_IDs):\n",
    "    '''\n",
    "        Given a list of document IDs, it outputs the document names corresponding to thos IDs.\n",
    "        Input: docID_to_docs (mapping between docID -> doc_name), docIDs - list of input document IDs\n",
    "        Returns: doc_names - list of doc_names corresponding to document IDs in doc_IDs\n",
    "    '''\n",
    "    doc_names = []\n",
    "    for doc_ID in doc_IDs:\n",
    "        doc_names.append(docID_to_doc[doc_ID])\n",
    "    return doc_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_toks = read_file(file_paths) #reading the files to collect tokens for all the documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#froming the total vocabulary using the individual document tokens\n",
    "vocabulary_set = set()\n",
    "for doc_tok in document_toks:\n",
    "    for tok in doc_tok:\n",
    "        vocabulary_set.add(tok)\n",
    "vocabulary_list = list(vocabulary_set)\n",
    "\n",
    "#creating term-id and id-term mapping\n",
    "id_to_term = {}\n",
    "term_to_id = {}\n",
    "for i in range(len(vocabulary_list)):\n",
    "    id_to_term[i] = vocabulary_list[i]\n",
    "    term_to_id[vocabulary_list[i]] = i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jaccard Coefficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_jaccard_coeff(query_toks, doc_toks):\n",
    "    '''\n",
    "        This function computes the Jaccard Coefficient of a Document and a Query given the document tokens and the query tokens.\n",
    "    '''\n",
    "    query_tok_set = set(query_toks)\n",
    "    doc_toks_set = set(doc_toks)\n",
    "    num_intersection = len(list(query_tok_set & doc_toks_set)) #intersection of document_tokens and query_tokens sets\n",
    "    num_union = len(list(query_tok_set | doc_toks_set)) #union of document_tokens and query_tokens sets\n",
    "    jaccard = num_intersection / num_union #calculating jaccard coefficient value\n",
    "    return jaccard\n",
    "\n",
    "def perform_jaccard_scoring(query_toks, document_toks):\n",
    "    '''\n",
    "        Calculates the Jaccard coefficient between a query and all the documents in our dataset. \n",
    "    '''\n",
    "    jaccard_coeff_values = {}\n",
    "    for i in range(len(document_toks)):\n",
    "        jaccard_coeff_i = compute_jaccard_coeff(query_toks, document_toks[i]) #calculate jaccard coefficient value b/w query and i_th document\n",
    "        jaccard_coeff_values[i] = jaccard_coeff_i\n",
    "    return jaccard_coeff_values\n",
    "\n",
    "def get_relevant_by_jaccard(jaccard_coeff_values):\n",
    "    '''\n",
    "        This function gives the top 5 relevant documents (IDs and Jaccard coefficient values) based on the value of the Jaccard coefficient.\n",
    "    '''\n",
    "    ranked_order_by_jaccard = dict(sorted(jaccard_coeff_values.items(), key=lambda item: item[1], reverse=True)) #sort in descending order based on the value of Jaccard coefficient\n",
    "    top_5_docID = list(ranked_order_by_jaccard.keys())[:5] #take top 5 document IDs from the descending sorted dictionary\n",
    "    top_5_jaccard = list(ranked_order_by_jaccard.values())[:5] #take corresponding top 5 Jaccard coefficient values from the descending sorted dictionary\n",
    "    return top_5_docID, top_5_jaccard\n",
    "\n",
    "def run_jaccard(query, document_toks):\n",
    "    '''\n",
    "        This function is used to run the whole process.\n",
    "    '''\n",
    "    query_toks = cleanQuery(query) #cleaning (preprocessing and tokenizing) the query\n",
    "    if(len(query_toks) == 0): #if the number of query tokens remaining after preprocessing is zero\n",
    "        print(\"no. of query tokens after preprocessing is 0. Jaccard coefficient with all documents is equal to 0\")\n",
    "        for i in range(5):\n",
    "            print(f\"{i + 1} : {docID_to_doc_mapping[i]} (0)\") \n",
    "    jaccard_scores = perform_jaccard_scoring(query_toks, document_toks) #calculate the Jaccard coefficient between a query and all the documents in our dataset\n",
    "    top_5_doc_ids, top_5_jaccard_score = get_relevant_by_jaccard(jaccard_scores) #get the top 5 relevant documents (IDs and Jaccard coefficient values) based on the value of the Jaccard coefficient.\n",
    "    top_5_doc_names = getDocsFromID(docID_to_doc_mapping, top_5_doc_ids) #get document names from their IDs\n",
    "    print(f\"Query Text = {query}\")\n",
    "    print(f\"Query tokens after preprocessing = {query_toks}\")\n",
    "    print(f\"Top 5 relevant documents based on the value of the Jaccard coefficient : \")\n",
    "    for i in range(len(top_5_doc_names)):\n",
    "        print(f\"{i + 1} : {top_5_doc_names[i]} ({top_5_jaccard_score[i]})\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_raw_term_frequency(document_toks):\n",
    "    '''\n",
    "        This function calculates the term frequency of each term present in a document. It returns a dictionary with key as document IDs. \n",
    "        Each key has a value equal to another dictionary of term-wise frequencies of terms present in that document.\n",
    "    '''\n",
    "    #Basically here we  calculate the raw count of the word in each document and stored it as a nested dictionary for each document.\n",
    "    raw_term_freq = {} #dictionary to hold term_frequency dictionary for each document\n",
    "    for i in range(len(document_toks)): #iterate over all the documents\n",
    "        raw_term_freq[i] = {}\n",
    "        unique_toks, tok_freq = np.unique(document_toks[i], return_counts=True) #calculate the unique tokens and its corresponding term frequency in the i_th document\n",
    "        for j in range(len(unique_toks)):\n",
    "            raw_term_freq[i][unique_toks[j]] = tok_freq[j] #fill the dictionary holding term frequencies for each document\n",
    "    return raw_term_freq\n",
    "\n",
    "def generate_term_postings(document_toks):\n",
    "    '''\n",
    "        This function generates the term-wise posting lists. Returns a dictionary corresponding to the posting list of all the terms \n",
    "        containg the document IDs correspondg to each term. \n",
    "    '''\n",
    "    term_posting_lists = {} #dictionary to hold the posting lists. Key is the term and value is the posting list.\n",
    "    for i in range(len(document_toks)): #Iterate over all the files\n",
    "        for tok in document_toks[i]: #For each file, iterate over all the file tokens\n",
    "            if(tok not in term_posting_lists.keys()): #if the token is not yet present as a term in the dict\n",
    "                term_posting_lists[tok] = [i] #create a new entry for the term in the index and intilize it with document_ID 'i'\n",
    "            else: #else if the token is already present as a term in the dict\n",
    "                if(i not in term_posting_lists[tok]): #if the document ID 'i' is not yet added to the posting list for the term \n",
    "                    term_posting_lists[tok].append(i)\n",
    "    return term_posting_lists\n",
    "\n",
    "def compute_document_frequency(term_posting_lists):\n",
    "    '''\n",
    "        This function computes the document frequency for each term using the term posting lists. Document frequency of a term is equal\n",
    "        to the no. of docs in its posting list\n",
    "    '''\n",
    "    term_df = {}\n",
    "    for term in term_posting_lists.keys(): #iterate over all the terms\n",
    "        term_df[term] = len(term_posting_lists[term]) #document frequency of a term is equal to the no. of docs in its posting list\n",
    "    return term_df\n",
    "\n",
    "def compute_IDF(term_df, num_total_docs):\n",
    "    '''\n",
    "        This function computes the inverse document frequency (idf) for each term given its document frequency (df)\n",
    "    '''\n",
    "    term_idf = {}\n",
    "    for term in term_df.keys(): #iterate over all the terms\n",
    "        idf_value = np.log10(num_total_docs / (term_df[term] + 1)) #Using formula IDF(word)=log(total no. of documents/document frequency(word)+1) [with 1-smoothing]\n",
    "        term_idf[term] = idf_value\n",
    "    return term_idf\n",
    "\n",
    "def compute_tf_weight(scheme, term, doc_tfs):\n",
    "    '''\n",
    "        This function calculates the TF-weight of a term in a document given the dictionary of term frequencies of all the terms in that document.\n",
    "        Input - scheme: denotes what weighting scheme to use, term- the term to calculate the TF-weight for, doc_tfs: dictionary of term frequencies of all the terms in that document\n",
    "    '''\n",
    "\n",
    "    #We have used the formulas provided in the assignment for each weighting sscheme to calculate TF-weight\n",
    "\n",
    "    if(scheme == \"binary\"): #if scheme is binary\n",
    "        if(term in doc_tfs.keys()): #if term is present in the document\n",
    "            return 1\n",
    "        else: #if term is not present in the document\n",
    "            return 0\n",
    "    elif(scheme == \"raw_count\"): #if scheme is raw_count\n",
    "        if(term in doc_tfs.keys()): #if term is present in the document\n",
    "            return doc_tfs[term]\n",
    "        else: #if term is not present in the document\n",
    "            return 0\n",
    "    elif(scheme == \"term_frequency\"): #if scheme is term_frequency\n",
    "        if(term in doc_tfs.keys()): #if term is present in the document\n",
    "            total_terms = sum(doc_tfs.values())\n",
    "            return doc_tfs[term] / total_terms\n",
    "        else: #if term is not present in the document\n",
    "            return 0\n",
    "    elif(scheme == \"log_normalization\"): #if scheme is log_normalization\n",
    "        if(term in doc_tfs.keys()): #if term is present in the document\n",
    "            return np.log10(1 + doc_tfs[term])\n",
    "        else: #if term is not present in the document\n",
    "            return 0\n",
    "    elif(scheme == \"double_normalization\"): #if scheme is double_normalization\n",
    "        if(term in doc_tfs.keys()): #if term is present in the document\n",
    "            t1 = 0.5\n",
    "            t2 = (0.5)*(doc_tfs[term] / max(doc_tfs.values()))\n",
    "            return t1 + t2\n",
    "        else: #if term is not present in the document\n",
    "            return 0.5\n",
    "\n",
    "def generate_tf_idf_matrices(document_toks, vocabulary_list):\n",
    "    '''\n",
    "        This function generates the TF-IDF matrices for all the schemes mentioned. All the TF-ODF matrices are stored in the dictionary \n",
    "        'tf_idf_matrix_by_scheme'. Function returns this dictionary along with two other dictionaries raw_tfs containing the raw term frequencies \n",
    "        for all terms in all documents and also the term_idfs\n",
    "    '''\n",
    "    num_docs = len(list(document_toks)) #number of docs\n",
    "    print(f\"Num docs = {num_docs}\")\n",
    "    raw_tfs = compute_raw_term_frequency(document_toks) #This function calculates the raw term frequency of each term present in each document\n",
    "    term_wise_postings = generate_term_postings(document_toks) #get the posting list for all the terms\n",
    "    term_document_freq = compute_document_frequency(term_wise_postings) #getting the document frequency for all terms\n",
    "    term_idfs = compute_IDF(term_document_freq, num_docs) #getting the idf for all terms\n",
    "    num_words = len(vocabulary_list) #number of unique words across all documents (vocabulary)\n",
    "    tf_idf_matrix_by_scheme = {} #dictionary to store all tf-idf matrices\n",
    "    schemes_list = ['binary', 'raw_count', 'term_frequency', 'log_normalization', 'double_normalization']\n",
    "    for scheme in schemes_list:\n",
    "        tf_idf_matrix_by_scheme[scheme] = np.zeros((num_docs, num_words)) #initializing the matrices of size (num_docs x  num_words_in_vocabulary)\n",
    "    for scheme in schemes_list: #loop to generate tf-idf matrix for each scheme\n",
    "        print(f\"Generating for scheme : {scheme}\")\n",
    "        for i in tqdm(range(num_docs)):\n",
    "            for j in range(num_words):\n",
    "                tf_weight = compute_tf_weight(scheme, id_to_term[j], raw_tfs[i]) #compute the TF-weight for a term in a document\n",
    "                idf = term_idfs[id_to_term[j]] #getting idf of the term\n",
    "                tf_idf_matrix_by_scheme[scheme][i][j] = tf_weight * idf #value in tf-idf matrix is product of wieght of tf with idf\n",
    "    return raw_tfs, term_idfs, tf_idf_matrix_by_scheme\n",
    "\n",
    "def get_query_vector(query_toks, scheme, term_idfs, vocab_len):\n",
    "    '''\n",
    "        This functions generates the query vector of size same as that of size of our vocabulary. Input is the query tokens and output is the query vector\n",
    "    '''\n",
    "    num_query_toks = len(query_toks) #number of tokens in the query\n",
    "    query_vector = [0] * vocab_len #initializing a query vector of length vocab size\n",
    "    query_tfs = {} #raw-tf of terms in query\n",
    "    for i in range(num_query_toks):\n",
    "        query_tfs[query_toks[i]] = 0\n",
    "    for i in range(num_query_toks):\n",
    "        query_tfs[query_toks[i]] += 1 #calculating raw-tf of terms in the query\n",
    "    for i in range(len(query_vector)):\n",
    "        term_tf_weight = compute_tf_weight(scheme, id_to_term[i], query_tfs) #computing the tf-weight for the terms wrt to query terms\n",
    "        if(id_to_term[i] not in term_idfs.keys()): #if the term is not in idf terms, idf_val is 0\n",
    "            idf_val = 0\n",
    "        else:\n",
    "            idf_val = term_idfs[id_to_term[i]] #else idf value is the value of idf wrt to documents we have\n",
    "        query_vector[i] = term_tf_weight * idf_val #tf-idf computation for the query vector\n",
    "    return query_vector\n",
    "\n",
    "def process_tf_idf_query(query, tf_idf_matrix_dict, idf_values, vocab_len):\n",
    "    '''\n",
    "        This function processes a query according to TF-IDF based scheme to find the top 5 relevant documents based on the TF-IDF score for a query. \n",
    "        This is done for all weighting schemes\n",
    "    '''\n",
    "    query_toks = cleanQuery(query) #get query tokens after preprocessing\n",
    "    print(f\"Query : {query}\")\n",
    "    print(f\"Query tokens = {query_toks}\\n\")\n",
    "    schemes_list = ['binary', 'raw_count', 'term_frequency', 'log_normalization', 'double_normalization'] #all weighting schemes\n",
    "    for scheme in schemes_list: #one by one perform this for all the weighting Sschemes\n",
    "        print(f\"\\n---------------- Scheme : {scheme} -------------------\\n\") \n",
    "        query_vector = np.array(get_query_vector(query_toks, scheme, idf_values, vocab_len)).reshape((vocab_len, 1)) # getting the |v|x1 query vector where |v| is size of vocab\n",
    "        tf_idf_matrix = np.array(tf_idf_matrix_dict[scheme]) # Getting the |d| x |v| tf-idf matrix for the chosen weighting scheme |d| -> no. of docs\n",
    "        document_scores = np.dot(tf_idf_matrix, query_vector) #computing the scores for all documents. The dot product between tf-idf matrix and query vector returns a |d| x 1 vector with scores wrt each document for a particular query.\n",
    "        document_id_to_score = {}\n",
    "        for i in range(document_scores.shape[0]): #mapping document id to their scores\n",
    "            document_id_to_score[i] = document_scores[i][0]\n",
    "        document_id_to_score = dict(sorted(document_id_to_score.items(), key=lambda item: item[1], reverse=True)) #sort the scores in descending order\n",
    "        top_5_doc_ids = list(document_id_to_score.keys())[:5] #choose the top 5 scores\n",
    "        top_5_doc_names = getDocsFromID(docID_to_doc_mapping, top_5_doc_ids) #getting doc names from ID\n",
    "        for i in range(5):\n",
    "            print(f\"{i} -> {top_5_doc_names[i]} | Score = {document_id_to_score[top_5_doc_ids[i]]}\")\n",
    "        print(\"-----------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num docs = 1133\n",
      "Generating for scheme : binary\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1133/1133 [02:30<00:00,  7.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating for scheme : raw_count\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1133/1133 [02:34<00:00,  7.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating for scheme : term_frequency\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1133/1133 [03:15<00:00,  5.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating for scheme : log_normalization\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1133/1133 [02:43<00:00,  6.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating for scheme : double_normalization\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1133/1133 [02:44<00:00,  6.88it/s]\n"
     ]
    }
   ],
   "source": [
    "raw_tf_docs, term_idfs, tf_idf_matrix_dict  = generate_tf_idf_matrices(document_toks, vocabulary_list)\n",
    "pickle.dump(tf_idf_matrix_dict, open('./tf_idf_matrices_q1.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_to_read = open(\"./tf_idf_matrices_q1.pkl\", \"rb\")\n",
    "tf_idf_matrix_dict = pickle.load(file_to_read)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_question1():\n",
    "    method_to_run = int(input(\"Enter 1 -> Jaccard scoring ; 2 -> TF-IDF : \"))\n",
    "    if(method_to_run == 1):\n",
    "        query_input_text = input(\"Input query text\")\n",
    "        if(query_input_text == \"\"):\n",
    "            print(\"Empty Query !!\")\n",
    "            return\n",
    "        else:\n",
    "            run_jaccard(query_input_text, document_toks)\n",
    "    else:\n",
    "        query_input_text = input(\"Input query text\")\n",
    "        if(query_input_text == \"\"):\n",
    "            print(\"Empty Query !!\")\n",
    "            return\n",
    "        else:\n",
    "            process_tf_idf_query(query_input_text, tf_idf_matrix_dict, term_idfs, len(vocabulary_list))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_question1()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query : good\n",
      "Query tokens = ['good']\n",
      "\n",
      "\n",
      "---------------- Scheme : binary -------------------\n",
      "\n",
      "0 -> 1st_aid.txt | Score = 0.10409730823826448\n",
      "1 -> abbott.txt | Score = 0.10409730823826448\n",
      "2 -> acronyms.txt | Score = 0.10409730823826448\n",
      "3 -> ads.txt | Score = 0.10409730823826448\n",
      "4 -> adt_miam.txt | Score = 0.10409730823826448\n",
      "-----------------------------------------------------------\n",
      "\n",
      "---------------- Scheme : raw_count -------------------\n",
      "\n",
      "0 -> manners.txt | Score = 8.848271200252482\n",
      "1 -> practica.txt | Score = 5.621254644866283\n",
      "2 -> oldtime.sng | Score = 4.684378870721901\n",
      "3 -> mlverb.hum | Score = 4.059795021292315\n",
      "4 -> vegan.rcp | Score = 3.8516004048157857\n",
      "-----------------------------------------------------------\n",
      "\n",
      "---------------- Scheme : term_frequency -------------------\n",
      "\n",
      "0 -> oldtime.sng | Score = 0.007173627673387291\n",
      "1 -> f_tang.txt | Score = 0.004053346515472245\n",
      "2 -> popmach | Score = 0.0033579776851053057\n",
      "3 -> bless.bc | Score = 0.0023658479145060108\n",
      "4 -> beer.hum | Score = 0.0022986787308134113\n",
      "-----------------------------------------------------------\n",
      "\n",
      "---------------- Scheme : log_normalization -------------------\n",
      "\n",
      "0 -> manners.txt | Score = 0.06062024096050611\n",
      "1 -> practica.txt | Score = 0.05453672269832806\n",
      "2 -> oldtime.sng | Score = 0.052104864881494094\n",
      "3 -> mlverb.hum | Score = 0.05020281233363449\n",
      "4 -> vegan.rcp | Score = 0.04950475004557565\n",
      "-----------------------------------------------------------\n",
      "\n",
      "---------------- Scheme : double_normalization -------------------\n",
      "\n",
      "0 -> curse.txt | Score = 133740.40166561474\n",
      "1 -> a_tv_t-p.com | Score = 133715.67273826225\n",
      "2 -> strine.txt | Score = 133713.7630335622\n",
      "3 -> growth.txt | Score = 133703.88595301862\n",
      "4 -> turbo.hum | Score = 133703.70962464626\n",
      "-----------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# input_query = input()\n",
    "# process_tf_idf_query(input_query, tf_idf_matrix_dict, term_idfs, len(vocabulary_list))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2370e07001db70a9c24f7e21173c51fbc4321340913a02830aed4885459fa0a0"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
