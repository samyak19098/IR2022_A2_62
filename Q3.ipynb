{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\Samyak\n",
      "[nltk_data]     Jain\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to C:\\Users\\Samyak\n",
      "[nltk_data]     Jain\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to C:\\Users\\Samyak\n",
      "[nltk_data]     Jain\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to C:\\Users\\Samyak\n",
      "[nltk_data]     Jain\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import copy\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import contractions\n",
    "import random\n",
    "from collections import Counter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = [\"comp.graphics\", \"sci.med\", \"talk.politics.misc\", \"rec.sport.hockey\", \"sci.space\"]\n",
    "class_labels = [0, 1, 2, 3, 4]\n",
    "class_name_to_label = {class_names[i]:i for i in range(len(class_names))}\n",
    "class_label_to_name = {i:class_names[i] for i in range(len(class_names))}\n",
    "data_folder = \"./data/20_newsgroups/20_newsgroups\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitData(class_names, data_folder, train_ratio):\n",
    "\n",
    "    class_wise_data = []\n",
    "    for i in range(len(class_names)):\n",
    "        class_dir = data_folder + '/' + class_names[i]\n",
    "        file_names = os.listdir(class_dir)\n",
    "        n_docs_class = len(file_names)\n",
    "        shuffled_docs = random.sample(file_names, n_docs_class)\n",
    "        n_train = int(train_ratio * n_docs_class)\n",
    "        n_test = n_docs_class - n_train\n",
    "        train_docs_class = shuffled_docs[:n_train]\n",
    "        test_docs_class = shuffled_docs[n_train:]\n",
    "        class_wise_data.append({'train' : train_docs_class, 'test' : test_docs_class})\n",
    "    return class_wise_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_wise_data = splitData(class_names, data_folder, 0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_alnum(tok):\n",
    "    '''\n",
    "        Remove non-alphanumeric characters from a string\n",
    "    '''\n",
    "\n",
    "    tok = ''.join(ch for ch in tok if ch.isalnum() == True)\n",
    "    return tok\n",
    "\n",
    "def remove_punct(tok):\n",
    "    '''\n",
    "        Remove the punctuation in token\n",
    "    '''\n",
    "    punctuations = string.punctuation\n",
    "    tok = ''.join(ch for ch in tok if ch not in punctuations)\n",
    "    return tok\n",
    "\n",
    "def remove_blank_space(tok):\n",
    "    '''\n",
    "        Remove the spaces in token\n",
    "    '''\n",
    "    tok = ''.join(ch for ch in tok if ch != ' ')\n",
    "    return tok\n",
    "def preprocess(text):\n",
    "\n",
    "    text = text.lower()\n",
    "\n",
    "    text = contractions.fix(text)\n",
    "\n",
    "    all_tokens = word_tokenize(text)\n",
    "\n",
    "    all_tokens = [check_alnum(tok) for tok in all_tokens]\n",
    "\n",
    "    stop_words = list(set(stopwords.words('english')))\n",
    "\n",
    "    all_tokens = [tok for tok in all_tokens if tok not in stop_words]\n",
    "\n",
    "    toks_no_punct = []\n",
    "    for tok in all_tokens:\n",
    "        ctok = remove_punct(tok)\n",
    "        if(ctok != \"\"):\n",
    "            toks_no_punct.append(ctok)\n",
    "\n",
    "    cleaned_toks = []\n",
    "    for tok in toks_no_punct:\n",
    "        ctok = remove_blank_space(tok)\n",
    "        if(ctok != \"\"):\n",
    "            cleaned_toks.append(ctok)\n",
    "    \n",
    "    final_tokens = [tok for tok in cleaned_toks]\n",
    "\n",
    "    return final_tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def readClassFiles(class_file_paths):\n",
    "#     class_doc_tokens = []\n",
    "#     for fpath in class_file_paths:\n",
    "#         f = open(fpath, 'r', encoding='utf-8', errors='ignore')\n",
    "#         ftxt_unprocessed = f.read()\n",
    "#         doc_toks = preprocess(ftxt_unprocessed)\n",
    "#         class_doc_tokens.append(doc_toks)\n",
    "#     return class_doc_tokens\n",
    "\n",
    "def process_data(class_wise_data, class_labels):\n",
    "    class_wise_train_unique_tokens = {}\n",
    "    class_wise_tokens = {i: {'train' : [], 'test': []} for i in range(5)}\n",
    "    class_wise_train_tfs = {}\n",
    "    for label in class_labels:\n",
    "        print(f\"--- Calculating for label = {label} ---\")\n",
    "        class_train_data = class_wise_data[label]['train']\n",
    "        class_test_data = class_wise_data[label]['test']\n",
    "        class_train_tokens = []\n",
    "        print(\">> reading through train files and preprocessing\")\n",
    "        for doc in tqdm(class_train_data):\n",
    "            f = open(data_folder + '/' + class_label_to_name[label] + '/' + doc, encoding='utf-8', errors='ignore')\n",
    "            ftxt_unproc = f.read()\n",
    "            doc_toks = preprocess(ftxt_unproc)\n",
    "            class_train_tokens.append(doc_toks)\n",
    "        print(\"--- Done\")\n",
    "        class_wise_tokens[label]['train'] = class_train_tokens\n",
    "        class_test_tokens = []\n",
    "        print(\">> reading through files and preprocessing\")\n",
    "        for doc in tqdm(class_test_data):\n",
    "            f = open(data_folder + '/' + class_label_to_name[label] + '/' + doc, encoding='utf-8', errors='ignore')\n",
    "            ftxt_unproc = f.read()\n",
    "            doc_toks = preprocess(ftxt_unproc)\n",
    "            class_test_tokens.append(doc_toks)\n",
    "        print(\"--- Done\")\n",
    "        class_wise_tokens[label]['test'] = class_test_tokens\n",
    "\n",
    "        # x = np.array(class_train_tokens[0])\n",
    "        # print(x.shape)\n",
    "        # all_class_train_tokens = list(np.array(class_train_tokens).flatten())\n",
    "        print(\">> Processing all tokens\")\n",
    "        all_class_train_tokens = []\n",
    "        for doc_toks in class_train_tokens:\n",
    "            for tok in doc_toks:\n",
    "                all_class_train_tokens.append(tok)\n",
    "        print(\"--- Done\")\n",
    "        # print(all_class_train_tokens.shape)\n",
    "        print(\">> Calculating class-wise TF\")\n",
    "        class_tfs = dict(Counter(all_class_train_tokens))\n",
    "        class_wise_train_tfs[label] = class_tfs\n",
    "        print(\"--- Done\")\n",
    "        class_wise_train_unique_tokens[label] = list(set(all_class_train_tokens))\n",
    "        print(\"\\n--------------------------\\n\\n\")\n",
    "    return class_wise_tokens, class_wise_train_unique_tokens, class_wise_train_tfs\n",
    "\n",
    "def compute_icf(class_wise_unq_toks):\n",
    "    all_class_toks = set()\n",
    "    for label in class_labels:\n",
    "        for tok in class_wise_unq_toks[label]:\n",
    "            all_class_toks.add(tok)\n",
    "        # all_class_toks.add(class_wise_unq_toks[label])\n",
    "    term_icf = {}\n",
    "    num_classes = len(class_labels)\n",
    "    for tok in all_class_toks:\n",
    "        present = 0\n",
    "        for label in class_labels:\n",
    "            if(tok in class_wise_unq_toks[label]):\n",
    "                present += 1\n",
    "        term_icf[tok] = np.log10(num_classes / present)\n",
    "    return term_icf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Calculating for label = 0 ---\n",
      ">> reading through train files and preprocessing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 800/800 [00:04<00:00, 165.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Done\n",
      ">> reading through files and preprocessing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:01<00:00, 187.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Done\n",
      ">> Processing all tokens\n",
      "--- Done\n",
      ">> Calculating class-wise TF\n",
      "--- Done\n",
      "\n",
      "--------------------------\n",
      "\n",
      "\n",
      "--- Calculating for label = 1 ---\n",
      ">> reading through train files and preprocessing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 800/800 [00:05<00:00, 148.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Done\n",
      ">> reading through files and preprocessing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:01<00:00, 163.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Done\n",
      ">> Processing all tokens\n",
      "--- Done\n",
      ">> Calculating class-wise TF\n",
      "--- Done\n",
      "\n",
      "--------------------------\n",
      "\n",
      "\n",
      "--- Calculating for label = 2 ---\n",
      ">> reading through train files and preprocessing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 800/800 [00:06<00:00, 114.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Done\n",
      ">> reading through files and preprocessing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:01<00:00, 105.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Done\n",
      ">> Processing all tokens\n",
      "--- Done\n",
      ">> Calculating class-wise TF\n",
      "--- Done\n",
      "\n",
      "--------------------------\n",
      "\n",
      "\n",
      "--- Calculating for label = 3 ---\n",
      ">> reading through train files and preprocessing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 800/800 [00:06<00:00, 133.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Done\n",
      ">> reading through files and preprocessing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:01<00:00, 160.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Done\n",
      ">> Processing all tokens\n",
      "--- Done\n",
      ">> Calculating class-wise TF\n",
      "--- Done\n",
      "\n",
      "--------------------------\n",
      "\n",
      "\n",
      "--- Calculating for label = 4 ---\n",
      ">> reading through train files and preprocessing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 800/800 [00:06<00:00, 132.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Done\n",
      ">> reading through files and preprocessing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:01<00:00, 142.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Done\n",
      ">> Processing all tokens\n",
      "--- Done\n",
      ">> Calculating class-wise TF\n",
      "--- Done\n",
      "\n",
      "--------------------------\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "class_wise_tokens, class_wise_train_unique_tokens, class_wise_train_tfs = process_data(class_wise_data, class_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "term_icfs = compute_icf(class_wise_train_unique_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'a': 3, 'b': 2, 'c': 1, 'd': 1, 'e': 1})\n",
      "[3, 1, 1, 1, 0, 0]\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def feature_selection"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2370e07001db70a9c24f7e21173c51fbc4321340913a02830aed4885459fa0a0"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
