{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\Samyak\n",
      "[nltk_data]     Jain\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to C:\\Users\\Samyak\n",
      "[nltk_data]     Jain\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to C:\\Users\\Samyak\n",
      "[nltk_data]     Jain\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to C:\\Users\\Samyak\n",
      "[nltk_data]     Jain\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import copy\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import contractions\n",
    "import random\n",
    "from collections import Counter\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = [\"comp.graphics\", \"sci.med\", \"talk.politics.misc\", \"rec.sport.hockey\", \"sci.space\"]\n",
    "class_labels = [0, 1, 2, 3, 4]\n",
    "class_name_to_label = {class_names[i]:i for i in range(len(class_names))}\n",
    "class_label_to_name = {i:class_names[i] for i in range(len(class_names))}\n",
    "data_folder = \"./data/20_newsgroups/20_newsgroups\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitData(class_names, data_folder, train_ratio):\n",
    "\n",
    "    class_wise_data = []\n",
    "    for i in range(len(class_names)):\n",
    "        class_dir = data_folder + '/' + class_names[i]\n",
    "        file_names = os.listdir(class_dir)\n",
    "        n_docs_class = len(file_names)\n",
    "        shuffled_docs = random.sample(file_names, n_docs_class)\n",
    "        n_train = int(train_ratio * n_docs_class)\n",
    "        n_test = n_docs_class - n_train\n",
    "        train_docs_class = shuffled_docs[:n_train]\n",
    "        test_docs_class = shuffled_docs[n_train:]\n",
    "        class_wise_data.append({'train' : train_docs_class, 'test' : test_docs_class})\n",
    "    return class_wise_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class_wise_data = splitData(class_names, data_folder, 0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_alnum(tok):\n",
    "    '''\n",
    "        Remove non-alphanumeric characters from a string\n",
    "    '''\n",
    "\n",
    "    tok = ''.join(ch for ch in tok if ch.isalnum() == True)\n",
    "    return tok\n",
    "\n",
    "def remove_punct(tok):\n",
    "    '''\n",
    "        Remove the punctuation in token\n",
    "    '''\n",
    "    punctuations = string.punctuation\n",
    "    tok = ''.join(ch for ch in tok if ch not in punctuations)\n",
    "    return tok\n",
    "\n",
    "def remove_blank_space(tok):\n",
    "    '''\n",
    "        Remove the spaces in token\n",
    "    '''\n",
    "    tok = ''.join(ch for ch in tok if ch != ' ')\n",
    "    return tok\n",
    "def preprocess(text):\n",
    "\n",
    "    text = text.lower()\n",
    "\n",
    "    text = contractions.fix(text)\n",
    "\n",
    "    all_tokens = word_tokenize(text)\n",
    "\n",
    "    all_tokens = [check_alnum(tok) for tok in all_tokens]\n",
    "\n",
    "    stop_words = list(set(stopwords.words('english')))\n",
    "\n",
    "    all_tokens = [tok for tok in all_tokens if tok not in stop_words]\n",
    "\n",
    "    toks_no_punct = []\n",
    "    for tok in all_tokens:\n",
    "        ctok = remove_punct(tok)\n",
    "        if(ctok != \"\"):\n",
    "            toks_no_punct.append(ctok)\n",
    "\n",
    "    cleaned_toks = []\n",
    "    for tok in toks_no_punct:\n",
    "        ctok = remove_blank_space(tok)\n",
    "        if(ctok != \"\"):\n",
    "            cleaned_toks.append(ctok)\n",
    "    \n",
    "    final_tokens = [tok for tok in cleaned_toks]\n",
    "\n",
    "    return final_tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(class_wise_data, class_labels):\n",
    "    class_wise_train_unique_tokens = {}\n",
    "    class_wise_tokens = {i: {'train' : [], 'test': []} for i in range(5)}\n",
    "    class_wise_train_tfs = {}\n",
    "    for label in class_labels:\n",
    "        print(f\"--- Calculating for label = {label} ---\")\n",
    "        class_train_data = class_wise_data[label]['train']\n",
    "        class_test_data = class_wise_data[label]['test']\n",
    "        class_train_tokens = []\n",
    "        print(\">> reading through train files and preprocessing\")\n",
    "        for doc in tqdm(class_train_data):\n",
    "            f = open(data_folder + '/' + class_label_to_name[label] + '/' + doc, encoding='utf-8', errors='ignore')\n",
    "            ftxt_unproc = f.read()\n",
    "            doc_toks = preprocess(ftxt_unproc)\n",
    "            class_train_tokens.append(doc_toks)\n",
    "        print(\"--- Done\")\n",
    "        class_wise_tokens[label]['train'] = class_train_tokens\n",
    "        class_test_tokens = []\n",
    "        print(\">> reading through files and preprocessing\")\n",
    "        for doc in tqdm(class_test_data):\n",
    "            f = open(data_folder + '/' + class_label_to_name[label] + '/' + doc, encoding='utf-8', errors='ignore')\n",
    "            ftxt_unproc = f.read()\n",
    "            doc_toks = preprocess(ftxt_unproc)\n",
    "            class_test_tokens.append(doc_toks)\n",
    "        print(\"--- Done\")\n",
    "        class_wise_tokens[label]['test'] = class_test_tokens\n",
    "\n",
    "        # x = np.array(class_train_tokens[0])\n",
    "        # print(x.shape)\n",
    "        # all_class_train_tokens = list(np.array(class_train_tokens).flatten())\n",
    "        print(\">> Processing all tokens\")\n",
    "        all_class_train_tokens = []\n",
    "        for doc_toks in class_train_tokens:\n",
    "            for tok in doc_toks:\n",
    "                all_class_train_tokens.append(tok)\n",
    "        print(\"--- Done\")\n",
    "        # print(all_class_train_tokens.shape)\n",
    "        print(\">> Calculating class-wise TF\")\n",
    "        class_tfs = dict(Counter(all_class_train_tokens))\n",
    "        class_wise_train_tfs[label] = class_tfs\n",
    "        print(\"--- Done\")\n",
    "        class_wise_train_unique_tokens[label] = list(set(all_class_train_tokens))\n",
    "        print(\"\\n--------------------------\\n\")\n",
    "    return class_wise_tokens, class_wise_train_unique_tokens, class_wise_train_tfs\n",
    "\n",
    "def compute_icf(class_wise_unq_toks):\n",
    "    all_class_toks = set()\n",
    "    for label in class_labels:\n",
    "        for tok in class_wise_unq_toks[label]:\n",
    "            all_class_toks.add(tok)\n",
    "        # all_class_toks.add(class_wise_unq_toks[label])\n",
    "    term_icf = {}\n",
    "    num_classes = len(class_labels)\n",
    "    for tok in all_class_toks:\n",
    "        present = 0\n",
    "        for label in class_labels:\n",
    "            if(tok in class_wise_unq_toks[label]):\n",
    "                present += 1\n",
    "        term_icf[tok] = np.log10(num_classes / present)\n",
    "    return term_icf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Calculating for label = 0 ---\n",
      ">> reading through train files and preprocessing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 800/800 [00:05<00:00, 155.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Done\n",
      ">> reading through files and preprocessing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:01<00:00, 166.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Done\n",
      ">> Processing all tokens\n",
      "--- Done\n",
      ">> Calculating class-wise TF\n",
      "--- Done\n",
      "\n",
      "--------------------------\n",
      "\n",
      "--- Calculating for label = 1 ---\n",
      ">> reading through train files and preprocessing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 800/800 [00:05<00:00, 148.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Done\n",
      ">> reading through files and preprocessing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:01<00:00, 133.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Done\n",
      ">> Processing all tokens\n",
      "--- Done\n",
      ">> Calculating class-wise TF\n",
      "--- Done\n",
      "\n",
      "--------------------------\n",
      "\n",
      "--- Calculating for label = 2 ---\n",
      ">> reading through train files and preprocessing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 800/800 [00:06<00:00, 116.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Done\n",
      ">> reading through files and preprocessing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:01<00:00, 105.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Done\n",
      ">> Processing all tokens\n",
      "--- Done\n",
      ">> Calculating class-wise TF\n",
      "--- Done\n",
      "\n",
      "--------------------------\n",
      "\n",
      "--- Calculating for label = 3 ---\n",
      ">> reading through train files and preprocessing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 800/800 [00:05<00:00, 157.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Done\n",
      ">> reading through files and preprocessing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:01<00:00, 144.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Done\n",
      ">> Processing all tokens\n",
      "--- Done\n",
      ">> Calculating class-wise TF\n",
      "--- Done\n",
      "\n",
      "--------------------------\n",
      "\n",
      "--- Calculating for label = 4 ---\n",
      ">> reading through train files and preprocessing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 800/800 [00:05<00:00, 149.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Done\n",
      ">> reading through files and preprocessing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:01<00:00, 143.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Done\n",
      ">> Processing all tokens\n",
      "--- Done\n",
      ">> Calculating class-wise TF\n",
      "--- Done\n",
      "\n",
      "--------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# class_wise_tokens, class_wise_train_unique_tokens, class_wise_train_tfs = process_data(class_wise_data, class_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# term_icfs = compute_icf(class_wise_train_unique_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_selection(term_icfs, class_wise_train_unique_tokens, class_wise_train_tfs, k):\n",
    "    class_wise_top_k = {}\n",
    "    total_feature_set = set()\n",
    "    for label in class_labels:\n",
    "        class_unique_toks = class_wise_train_unique_tokens[label]\n",
    "        class_tfs = class_wise_train_tfs[label]\n",
    "        tf_icf_score = {}\n",
    "        for tok in class_unique_toks:\n",
    "            tf_icf_tok = class_tfs[tok] * term_icfs[tok]\n",
    "            tf_icf_score[tok] = tf_icf_tok\n",
    "        sorted_tf_icf = dict(sorted(tf_icf_score.items(), key=lambda item: item[1], reverse=True))\n",
    "        top_k_class_features = list(sorted_tf_icf.keys())[:k]\n",
    "        class_wise_top_k[label] = top_k_class_features\n",
    "        total_feature_set.update(top_k_class_features)\n",
    "    return class_wise_top_k, total_feature_set\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class_wise_top_k, feature_set = feature_selection(term_icfs, class_wise_train_unique_tokens, class_wise_train_tfs, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def featurize_data(class_wise_tokens, feature_set):\n",
    "    train_x = []\n",
    "    train_y = []\n",
    "    test_x = []\n",
    "    test_y = []\n",
    "\n",
    "    for label in class_labels:\n",
    "        \n",
    "        class_train_doc = class_wise_tokens[label]['train']\n",
    "        class_test_doc = class_wise_tokens[label]['test']\n",
    "\n",
    "        for doc_toks in class_train_doc:\n",
    "            doc_feature = []\n",
    "            # print(len(doc_toks))\n",
    "            doc_tfs = dict(Counter(doc_toks))\n",
    "            # toks_num = 0\n",
    "            # for k in doc_tfs.keys():\n",
    "            #     toks_num += doc_tfs[k]\n",
    "            # print(toks_num)\n",
    "            # print(doc_tfs)\n",
    "            # print(\"-----\\n\")\n",
    "            for i, tok in enumerate(feature_set):\n",
    "                if(tok in doc_tfs.keys()):\n",
    "                    doc_feature.append(doc_tfs[tok])\n",
    "                else:\n",
    "                    doc_feature.append(0)\n",
    "            train_x.append(doc_feature)\n",
    "            train_y.append(label)\n",
    "        \n",
    "        for doc_toks in class_test_doc:\n",
    "            doc_feature = []\n",
    "            doc_tfs = dict(Counter(doc_toks))\n",
    "            for i, tok in enumerate(feature_set):\n",
    "                if(tok in doc_tfs.keys()):\n",
    "                    doc_feature.append(doc_tfs[tok])\n",
    "                else:\n",
    "                    doc_feature.append(0)\n",
    "            test_x.append(doc_feature)\n",
    "            test_y.append(label)\n",
    "    \n",
    "    return train_x, test_x, train_y, test_y\n",
    "        \n",
    "        #featurizing class train data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_x, test_x, train_y, test_y = featurize_data(class_wise_tokens, feature_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_naive_bayes(train_x, train_y):\n",
    "    prior_prob = {}\n",
    "    conditional_prob = {label:{} for label in class_labels}\n",
    "    class_feature_cum = {label:{feat:0 for feat in range(len(train_x[0]))} for label in class_labels}\n",
    "    # print(conditional_prob)\n",
    "    total_class_samples = len(train_y)\n",
    "    num_features = len(train_x[0])\n",
    "\n",
    "    class_wise_count = dict(Counter(train_y))\n",
    "    for label in class_labels:\n",
    "        prior_prob[label] = float(class_wise_count[label]) / float(total_class_samples)\n",
    "    for i in range(total_class_samples):\n",
    "        sample_label = train_y[i]\n",
    "        for j in range(num_features):\n",
    "            class_feature_cum[sample_label][j] += train_x[i][j]\n",
    "    alpha = 1 #laplace add one smoothing\n",
    "    for label in class_labels:\n",
    "        for feature in range(num_features):\n",
    "            conditional_prob[label][feature] = float(class_feature_cum[label][feature] + alpha ) / float(sum(class_feature_cum[label].values()) + (num_features*alpha))\n",
    "    \n",
    "    return prior_prob, conditional_prob\n",
    "\n",
    "def predict_naive_bayes(test_x, prior_prob, conditional_prob):\n",
    "\n",
    "    predictions = []\n",
    "    for sample in test_x:\n",
    "        posterior_probs = {}\n",
    "        for label in class_labels:\n",
    "            probab = np.log10(prior_prob[label])\n",
    "            for feature in range(len(sample)):\n",
    "                if(sample[feature] != 0):\n",
    "                    probab += np.log10(conditional_prob[label][feature])\n",
    "            posterior_probs[label] = probab\n",
    "        pred_label = max(posterior_probs, key= lambda x: posterior_probs[x])\n",
    "        predictions.append(pred_label)\n",
    "    return predictions\n",
    "\n",
    "def compute_accuracy(true_y, pred_y):\n",
    "    correct = 0\n",
    "    total = len(true_y)\n",
    "    for i in range(total):\n",
    "        if(true_y[i] == pred_y[i]):\n",
    "            correct += 1\n",
    "    accuracy = float(correct) / float(total)\n",
    "    return accuracy\n",
    "\n",
    "def calculate_confusion_matrix(true_y, pred_y):\n",
    "    conf_matrix = np.zeros((len(class_labels), len(class_labels)))\n",
    "    for i in range(len(true_y)):\n",
    "        conf_matrix[true_y[i]][pred_y[i]] += 1\n",
    "    return conf_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prior_prob, conditional_prob = train_naive_bayes(train_x, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preds = predict_naive_bayes(test_x, prior_prob, conditional_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.462\n"
     ]
    }
   ],
   "source": [
    "# print(compute_accuracy(test_y, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_question3():\n",
    "    train_ratio = float(input(\"Enter train split ratio [b/w 0 and 1]: \"))\n",
    "    class_wise_data = splitData(class_names, data_folder, train_ratio)\n",
    "    k = int(input(\"Enter the value of k : \"))\n",
    "    print(\">> Processing Data\")\n",
    "    class_wise_tokens, class_wise_train_unique_tokens, class_wise_train_tfs = process_data(class_wise_data, class_labels)\n",
    "    print(\">> Computing term ICFs\")\n",
    "    term_icfs = compute_icf(class_wise_train_unique_tokens)\n",
    "    print(\">> Performing feature selection\")\n",
    "    class_wise_top_k, feature_set = feature_selection(term_icfs, class_wise_train_unique_tokens, class_wise_train_tfs, k)\n",
    "    print(\">> Featurizing dataset\")\n",
    "    train_x, test_x, train_y, test_y = featurize_data(class_wise_tokens, feature_set)\n",
    "    print(\">> Training Naive Bayes Model\")\n",
    "    prior_prob, conditional_prob = train_naive_bayes(train_x, train_y)\n",
    "    print(\">> Testing Naive Bayes Model\")\n",
    "    predicted_labels = predict_naive_bayes(test_x, prior_prob, conditional_prob)\n",
    "    accuracy_value = compute_accuracy(test_y, predicted_labels)\n",
    "    print(f\"Accuracy = {accuracy_value * 100}%\")\n",
    "    print(f\"SKLEARN ACC = {accuracy_score(test_y, predicted_labels)}\")\n",
    "    conf_matrix = calculate_confusion_matrix(test_y, predicted_labels)\n",
    "    print(f\"Confusion Matrix = {conf_matrix}\")\n",
    "    print(f\"SKLEARN CONF MATRIX = {confusion_matrix(test_y, predicted_labels)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Processing Data\n",
      "--- Calculating for label = 0 ---\n",
      ">> reading through train files and preprocessing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 700/700 [00:04<00:00, 149.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Done\n",
      ">> reading through files and preprocessing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 300/300 [00:02<00:00, 141.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Done\n",
      ">> Processing all tokens\n",
      "--- Done\n",
      ">> Calculating class-wise TF\n",
      "--- Done\n",
      "\n",
      "--------------------------\n",
      "\n",
      "--- Calculating for label = 1 ---\n",
      ">> reading through train files and preprocessing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 700/700 [00:05<00:00, 138.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Done\n",
      ">> reading through files and preprocessing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 300/300 [00:02<00:00, 121.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Done\n",
      ">> Processing all tokens\n",
      "--- Done\n",
      ">> Calculating class-wise TF\n",
      "--- Done\n",
      "\n",
      "--------------------------\n",
      "\n",
      "--- Calculating for label = 2 ---\n",
      ">> reading through train files and preprocessing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 700/700 [00:06<00:00, 101.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Done\n",
      ">> reading through files and preprocessing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 300/300 [00:02<00:00, 105.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Done\n",
      ">> Processing all tokens\n",
      "--- Done\n",
      ">> Calculating class-wise TF\n",
      "--- Done\n",
      "\n",
      "--------------------------\n",
      "\n",
      "--- Calculating for label = 3 ---\n",
      ">> reading through train files and preprocessing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 700/700 [00:04<00:00, 158.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Done\n",
      ">> reading through files and preprocessing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 300/300 [00:02<00:00, 122.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Done\n",
      ">> Processing all tokens\n",
      "--- Done\n",
      ">> Calculating class-wise TF\n",
      "--- Done\n",
      "\n",
      "--------------------------\n",
      "\n",
      "--- Calculating for label = 4 ---\n",
      ">> reading through train files and preprocessing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 700/700 [00:05<00:00, 136.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Done\n",
      ">> reading through files and preprocessing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 300/300 [00:02<00:00, 146.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Done\n",
      ">> Processing all tokens\n",
      "--- Done\n",
      ">> Calculating class-wise TF\n",
      "--- Done\n",
      "\n",
      "--------------------------\n",
      "\n",
      ">> Computing term ICFs\n",
      ">> Performing feature selection\n",
      ">> Featurizing dataset\n",
      ">> Training Naive Bayes Model\n",
      ">> Testing Naive Bayes Model\n",
      "Accuracy = 99.6%\n",
      "SKLEARN ACC = 0.996\n",
      "Confusion Matrix = [[299.   1.   0.   0.   0.]\n",
      " [  2. 298.   0.   0.   0.]\n",
      " [  0.   0. 298.   0.   2.]\n",
      " [  0.   0.   0. 300.   0.]\n",
      " [  1.   0.   0.   0. 299.]]\n",
      "SKLEARN CONF MATRIX = [[299   1   0   0   0]\n",
      " [  2 298   0   0   0]\n",
      " [  0   0 298   0   2]\n",
      " [  0   0   0 300   0]\n",
      " [  1   0   0   0 299]]\n"
     ]
    }
   ],
   "source": [
    "run_question3()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2370e07001db70a9c24f7e21173c51fbc4321340913a02830aed4885459fa0a0"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
