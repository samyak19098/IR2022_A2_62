{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\Samyak\n",
      "[nltk_data]     Jain\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to C:\\Users\\Samyak\n",
      "[nltk_data]     Jain\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to C:\\Users\\Samyak\n",
      "[nltk_data]     Jain\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to C:\\Users\\Samyak\n",
      "[nltk_data]     Jain\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import copy\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import contractions\n",
    "import random\n",
    "from collections import Counter\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "import math\n",
    "import seaborn as sbn\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = [\"comp.graphics\", \"sci.med\", \"talk.politics.misc\", \"rec.sport.hockey\", \"sci.space\"] #the five classes we have\n",
    "class_labels = [0, 1, 2, 3, 4] #assigning an integer label to each class\n",
    "class_name_to_label = {class_names[i]:i for i in range(len(class_names))} #class name to class label mapping\n",
    "class_label_to_name = {i:class_names[i] for i in range(len(class_names))} #class label to class name mapping\n",
    "data_folder = \"../data/20_newsgroups/20_newsgroups\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitData(class_names, data_folder, train_ratio):\n",
    "    '''\n",
    "        This function is used for splitting the data into train and test parts according to the train-test ratio.\n",
    "    '''\n",
    "    class_wise_data = [] # A list of dictionaries to hold the training and testing documents for each class.\n",
    "    for i in range(len(class_names)):\n",
    "        class_dir = data_folder + '/' + class_names[i] #directory for that class in the data\n",
    "        file_names = os.listdir(class_dir) #file names in that class directory\n",
    "        n_docs_class = len(file_names) #number of docs in the class\n",
    "        shuffled_docs = random.sample(file_names, n_docs_class) #shuffle the documents in the class so that when splitting we take the documents randomly into train and test sets and not sequentially.\n",
    "        n_train = int(train_ratio * n_docs_class) #number of train set docs\n",
    "        n_test = n_docs_class - n_train #number of test set docs\n",
    "        train_docs_class = shuffled_docs[:n_train] #the documents in the training set\n",
    "        test_docs_class = shuffled_docs[n_train:] #the documents in the testing set\n",
    "        class_wise_data.append({'train' : train_docs_class, 'test' : test_docs_class}) #Appending the training and testing doc lists as a dictionary to the total class-wise dataset list\n",
    "    return class_wise_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_alnum(tok):\n",
    "    '''\n",
    "        Remove non-alphanumeric characters from a string\n",
    "    '''\n",
    "\n",
    "    tok = ''.join(ch for ch in tok if ch.isalnum() == True)\n",
    "    return tok\n",
    "\n",
    "def remove_punct(tok):\n",
    "    '''\n",
    "        Remove the punctuation in token\n",
    "    '''\n",
    "    punctuations = string.punctuation\n",
    "    tok = ''.join(ch for ch in tok if ch not in punctuations)\n",
    "    return tok\n",
    "\n",
    "def remove_blank_space(tok):\n",
    "    '''\n",
    "        Remove the spaces in token\n",
    "    '''\n",
    "    tok = ''.join(ch for ch in tok if ch != ' ')\n",
    "    return tok\n",
    "def preprocess(text):\n",
    "    '''\n",
    "        Preprocess the file text and converting to word tokens\n",
    "        Input: string File text\n",
    "        Returns file_tokens, word tokens for the pre processed text\n",
    "    '''\n",
    "    #converting text to lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    #Fixing the contractions\n",
    "    text = contractions.fix(text)\n",
    "\n",
    "    #Performing word tokenization\n",
    "    all_tokens = word_tokenize(text)\n",
    "\n",
    "    #Omitting all the non-alphanumeric characters in tokens\n",
    "    all_tokens = [check_alnum(tok) for tok in all_tokens]\n",
    "\n",
    "    #removing stopwords\n",
    "    stop_words = list(set(stopwords.words('english')))\n",
    "    all_tokens = [tok for tok in all_tokens if tok not in stop_words]\n",
    "\n",
    "    #removing punctations if any remain\n",
    "    toks_no_punct = []\n",
    "    for tok in all_tokens:\n",
    "        ctok = remove_punct(tok)\n",
    "        if(ctok != \"\"):\n",
    "            toks_no_punct.append(ctok)\n",
    "\n",
    "    #removing spaces in any remain\n",
    "    cleaned_toks = []\n",
    "    for tok in toks_no_punct:\n",
    "        ctok = remove_blank_space(tok)\n",
    "        if(ctok != \"\"):\n",
    "            cleaned_toks.append(ctok)\n",
    "    \n",
    "    #stemming\n",
    "    stemmer = nltk.PorterStemmer()\n",
    "    final_tokens = [stemmer.stem(tok) for tok in cleaned_toks]\n",
    "\n",
    "    # final_tokens = [tok for tok in cleaned_toks]\n",
    "\n",
    "    return final_tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(class_wise_data, class_labels):\n",
    "    '''\n",
    "        This function takes the input of class_wise_data which is a list of dicts containing the training and testing set documents \n",
    "        belonging to each class. It returns three dictionaries: class_wise_tokens - nested dictionary with the outer dictionary having the\n",
    "        key as the class_label and the value as another dictionary that contains the tokens for each preprocessed document in the training and testing \n",
    "        set of that class ; class_wise_train_unique_tokens - A dictionary with key as the class_label and value is a list of all the unique tokens present \n",
    "        in the documents of that particular class; class_wise_train_tfs -  A dictionary with key as the class_label and value is a a dictionary which has the\n",
    "        term frequencies of all the unique terms in that class.\n",
    "    '''\n",
    "    class_wise_train_unique_tokens = {} #A dictionary with key as the class_label and value is a list of all the unique tokens present in the documents of that particular class\n",
    "    class_wise_tokens = {i: {'train' : [], 'test': []} for i in range(5)} #nested dictionary with the outer dictionary having the key as the class_label and the value as another dictionary that contains the tokens for each preprocessed document in the training and testing  set of that class\n",
    "    class_wise_train_tfs = {} #A dictionary with key as the class_label and value is a a dictionary which has the term frequencies of all the unique terms in that class\n",
    "    for label in tqdm(class_labels): #loop over all the class_labels\n",
    "        class_train_data = class_wise_data[label]['train'] #training set docs of that class\n",
    "        class_test_data = class_wise_data[label]['test'] #testing set docs of that class\n",
    "        class_train_tokens = [] #the document wise tokens for the training set docs in that class\n",
    "        for doc in class_train_data:\n",
    "            f = open(data_folder + '/' + class_label_to_name[label] + '/' + doc, encoding='utf-8', errors='ignore')\n",
    "            ftxt_unproc = f.read()\n",
    "            doc_toks = preprocess(ftxt_unproc) #preprocess the document to get tokens for that document\n",
    "            class_train_tokens.append(doc_toks)\n",
    "        class_wise_tokens[label]['train'] = class_train_tokens\n",
    "        class_test_tokens = [] #the document wise tokens for the testing set docs in that class\n",
    "        for doc in class_test_data:\n",
    "            f = open(data_folder + '/' + class_label_to_name[label] + '/' + doc, encoding='utf-8', errors='ignore')\n",
    "            ftxt_unproc = f.read()\n",
    "            doc_toks = preprocess(ftxt_unproc) #preprocess the document to get tokens for that document\n",
    "            class_test_tokens.append(doc_toks)\n",
    "        class_wise_tokens[label]['test'] = class_test_tokens\n",
    "\n",
    "        all_class_train_tokens = [] #all the tokens across all the training documents of the aparticular class\n",
    "        for doc_toks in class_train_tokens: #loop over all training doc's token lists\n",
    "            for tok in doc_toks:\n",
    "                all_class_train_tokens.append(tok)\n",
    "        class_tfs = dict(Counter(all_class_train_tokens)) #the term frequency for the terms in all the training document combined in that particular class.\n",
    "        #Note that here, term frequency of a term in a class is number of occurrences of a term in all documents (here train) of a particular class\n",
    "        class_wise_train_tfs[label] = class_tfs\n",
    "        class_wise_train_unique_tokens[label] = list(set(all_class_train_tokens)) #the unique tokens present across all the training docs combined of that particular class\n",
    "    return class_wise_tokens, class_wise_train_unique_tokens, class_wise_train_tfs\n",
    "\n",
    "def compute_icf(class_wise_unq_toks):\n",
    "    '''\n",
    "        This function computes the icf all the tokens present int the training set documents.\n",
    "    '''\n",
    "    all_class_toks = {} # Dictionary which holds the tokens present in train docs over all classes as keys. The value for each key is a list of all the classes the token/term appears in.\n",
    "    for label in class_labels: #iterate over all classes\n",
    "        for tok in class_wise_unq_toks[label]: #iterate over all the unique tokens of each class\n",
    "            if(tok in all_class_toks.keys()): #if token is present in the all_class_toks\n",
    "                all_class_toks[tok].append(label) #append current class' label to its list\n",
    "            else:\n",
    "                all_class_toks[tok] = [label] #else initialize its list\n",
    "    num_classes = len(class_labels) #number of classes\n",
    "    term_icf = {} #dict to hold the icf for each term in the training documents over all classes\n",
    "    for term in all_class_toks.keys():\n",
    "        assert (len(all_class_toks[term]) > 0 and len(all_class_toks[term]) <= 5)\n",
    "        class_frequency = len(all_class_toks[term]) #class frequency is the length of the list for a token which contains the class labels for the classes that token is present in.\n",
    "        term_icf[term] = math.log10(num_classes / class_frequency) #icf value of a term is ratio of total number of classes to the number of classes the term appears in\n",
    "    return term_icf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_selection(term_icfs, class_wise_train_unique_tokens, class_wise_train_tfs, k):\n",
    "    '''\n",
    "        Implements the feature selection algorithm to select top k features from each class. Returns a dictionary with the key as class labels \n",
    "        and value is a list of top-k features of that class. Also returns a set which is the union of top-k features from each class i.e\n",
    "        it is the effective vocabulary.\n",
    "        Note: Feature selection is to be done over only the training dataset.\n",
    "    '''\n",
    "    class_wise_top_k = {} #dictionary holding class-wise top k features\n",
    "    total_feature_set = set() #set holding the union of top-k features for each class i.e effective vocabulary\n",
    "    for label in class_labels: #loop over the all the labels\n",
    "        class_unique_toks = class_wise_train_unique_tokens[label] #unique tokens present in the training set of that class\n",
    "        class_tfs = class_wise_train_tfs[label] #term frequencies of the terms present in the training set of that class\n",
    "        tf_icf_score = {} #tf_icf_score for the terms in that class\n",
    "        for tok in class_unique_toks: #iterate over all the unique tokens present in a particular class\n",
    "            tf_icf_tok = class_tfs[tok] * term_icfs[tok] #tf-icf score is the product of the term frequency of a term in the training set of the class\n",
    "            tf_icf_score[tok] = tf_icf_tok\n",
    "        sorted_tf_icf = dict(sorted(tf_icf_score.items(), key=lambda item: item[1], reverse=True)) #sort the tf-icf scores of the terms in descing order\n",
    "        top_k_class_features = list(sorted_tf_icf.keys())[:k] #select the top k terms with highest tf-icf score in the class\n",
    "        class_wise_top_k[label] = top_k_class_features #these are the top-k features of that particular class\n",
    "        total_feature_set.update(top_k_class_features) #update the combined feature set (effective vocab) with these top k features of this class\n",
    "    return class_wise_top_k, total_feature_set\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def featurize_data(class_wise_tokens, feature_set):\n",
    "    '''\n",
    "        Featurizing the dataset i.e making the train_x, train_y, test_x, test_y according to the features we have selected.\n",
    "    '''\n",
    "\n",
    "    #Basically through this process we make the training X and Y, testing X and Y having those features that we have selected through the feature selection process\n",
    "\n",
    "    train_x = [] #holds the samples(feature values) for the training set\n",
    "    train_y = [] #holds the class labels corresponding to the samples of the training set\n",
    "    test_x = [] #holds the samples(feature values) for the testing set\n",
    "    test_y = [] #holds the class labels corresponding to the samples of the testing set\n",
    "\n",
    "    for label in class_labels: #iterate over all the class labels\n",
    "        \n",
    "        class_train_doc = class_wise_tokens[label]['train'] #document-wise tokens for the documents in the training set of the particular class\n",
    "        class_test_doc = class_wise_tokens[label]['test'] #document-wise tokens for the documents in the testing set of the particular class\n",
    "\n",
    "        for doc_toks in class_train_doc: #iterate over the list of document-wise tokens for the documents in the training set of the particular class\n",
    "            doc_feature = []\n",
    "            doc_tfs = dict(Counter(doc_toks)) #Term frequencies in a document for the tokens in the training doc\n",
    "            for i, tok in enumerate(feature_set): #iterate over the tokens/words in combined feature set/effective vocab\n",
    "                if(tok in doc_tfs.keys()): #if the word is present in this training document\n",
    "                    doc_feature.append(doc_tfs[tok]) #the feature value will be the term frequency of this term in this training document\n",
    "                else: #if the word is present in this training document\n",
    "                    doc_feature.append(0) #the feature value will be zero\n",
    "            train_x.append(doc_feature)\n",
    "            train_y.append(label)\n",
    "        \n",
    "        #repeat the same process for the testing set documents\n",
    "        for doc_toks in class_test_doc:\n",
    "            doc_feature = []\n",
    "            doc_tfs = dict(Counter(doc_toks))\n",
    "            for i, tok in enumerate(feature_set):\n",
    "                if(tok in doc_tfs.keys()):\n",
    "                    doc_feature.append(doc_tfs[tok])\n",
    "                else:\n",
    "                    doc_feature.append(0)\n",
    "            test_x.append(doc_feature)\n",
    "            test_y.append(label)\n",
    "    \n",
    "    return train_x, test_x, train_y, test_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_naive_bayes(train_x, train_y):\n",
    "    '''\n",
    "        This function is used for training of the Naive Bayes Model. Returns the prior and class conditional probabilities\n",
    "    '''\n",
    "    num_features = len(train_x[0])\n",
    "    prior_prob = {} #A dictionary to store the prior probabilities of the classes\n",
    "    conditional_prob = {label:{} for label in class_labels} #dictionary to store the class conditional pro\n",
    "    class_feature_cum = {label:{feat:0 for feat in range(num_features)} for label in class_labels} #cumulative class-wise stats for each selected feature.\n",
    "    #this is a nested dictionary where the outer dictionary has key as the class_label and value as another dictionary which stores the frequency of each feature(word) in that class.\n",
    "    total_class_samples = len(train_y)\n",
    "    class_wise_count = dict(Counter(train_y)) #class-wise count of samples ie- no. of training sample of each class\n",
    "    for label in class_labels: #iterate over all class labels\n",
    "        prior_prob[label] = float(class_wise_count[label]) / float(total_class_samples) #prior probab of class is ratio of the class_samples to the total_number_of_samples\n",
    "    for i in range(total_class_samples): #this loop constructs the class_feature_cum dictionary which will be used for conditional probability calculation\n",
    "        sample_label = train_y[i]\n",
    "        for j in range(num_features):\n",
    "            class_feature_cum[sample_label][j] += train_x[i][j]\n",
    "    alpha = 1 #for laplace add one smoothing\n",
    "    for label in class_labels:\n",
    "        for feature in range(num_features):\n",
    "            conditional_prob[label][feature] = float(class_feature_cum[label][feature] + alpha ) / float(sum(class_feature_cum[label].values()) + (num_features*alpha)) #conditional probab of a feature(word) wrt a class is the ratio of number of occurences of that word in that particular class divided by the sum of frequencies of all features(words) wrt that class.\n",
    "    \n",
    "    return prior_prob, conditional_prob\n",
    "\n",
    "def predict_naive_bayes(test_x, prior_prob, conditional_prob):\n",
    "    '''\n",
    "        This function is used for calculating the predicitons for a trained Naive Bayes model. Input is the testing set samples and the trained model's\n",
    "        prior probability and conditional probabilities.\n",
    "    '''\n",
    "\n",
    "    #Note: Here instead of calculating the posterior we calculate the log of posterior (doesn't affect the answer because log is increasing function). \n",
    "    # This is done to prevent underflow due to multiplication of very small probability values. Thus the multiplication of probabilities effectively transforms to\n",
    "    # addition of log probabilities.\n",
    "    predictions = []\n",
    "    for sample in test_x: #for each sample in the testing set\n",
    "        posterior_probs = {} #dictionary to hold the posterior probability for each class\n",
    "        for label in class_labels: #iterate over all the labels\n",
    "            probab = math.log10(prior_prob[label])  #prior probability part\n",
    "            for feature in range(len(sample)): #for each feature in feature set\n",
    "                if(sample[feature] != 0): #if the feature value is non zero i.e the word is present in the testing sample\n",
    "                    probab += (math.log10(conditional_prob[label][feature]) * sample[feature]) #we add the log conditional probability of that feature 'x' times where 'x' is the frequency of that word in the sample\n",
    "            posterior_probs[label] = probab\n",
    "        pred_label = max(posterior_probs, key= lambda x: posterior_probs[x]) #the predicted label is the key value corresponding to maximum posterior probability\n",
    "        predictions.append(pred_label)\n",
    "    return predictions\n",
    "\n",
    "def compute_accuracy(true_y, pred_y):\n",
    "    '''\n",
    "        Function to compute accuracy\n",
    "    '''\n",
    "    correct = 0\n",
    "    total = len(true_y)\n",
    "    for i in range(total):\n",
    "        if(true_y[i] == pred_y[i]):\n",
    "            correct += 1\n",
    "    accuracy = float(correct) / float(total)\n",
    "    return accuracy\n",
    "\n",
    "def calculate_confusion_matrix(true_y, pred_y):\n",
    "    '''\n",
    "        Function to calculate confusion matrix\n",
    "    '''\n",
    "    conf_matrix = np.zeros((len(class_labels), len(class_labels)))\n",
    "    for i in range(len(true_y)):\n",
    "        conf_matrix[true_y[i]][pred_y[i]] += 1\n",
    "    return conf_matrix\n",
    "\n",
    "def plot_confusion_matrix(conf_matrix, fig_name, save=False):\n",
    "    plt.figure(figsize=(8,6))\n",
    "    sbn.heatmap(conf_matrix, annot=True)\n",
    "    plt.ylabel(\"True label\")\n",
    "    plt.xlabel(\"Predicted label\")\n",
    "    if(save == True):\n",
    "        plt.savefig(fig_name, bbox_inches='tight', facecolor='w')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_question3(train_ratio, k):\n",
    "        \n",
    "    class_wise_data = splitData(class_names, data_folder, train_ratio)\n",
    "    print(\">> Processing Data\")\n",
    "    class_wise_tokens, class_wise_train_unique_tokens, class_wise_train_tfs = process_data(class_wise_data, class_labels)\n",
    "\n",
    "    print(\">> Computing term ICFs\")\n",
    "    term_icfs = compute_icf(class_wise_train_unique_tokens)\n",
    "\n",
    "    print(\">> Performing feature selection\")\n",
    "    class_wise_top_k, feature_set = feature_selection(term_icfs, class_wise_train_unique_tokens, class_wise_train_tfs, k)\n",
    "\n",
    "    # print(f\"\\nAfter selecting top {k} features from each class, Total Feature-set(Vocab) size = {len(feature_set)}\\n\")\n",
    "    \n",
    "    print(\">> Featurizing dataset\")\n",
    "    train_x, test_x, train_y, test_y = featurize_data(class_wise_tokens, feature_set)\n",
    "\n",
    "    print(\">> Training Naive Bayes Model\")\n",
    "    prior_prob, conditional_prob = train_naive_bayes(train_x, train_y)\n",
    "\n",
    "    print(\">> Testing Naive Bayes Model\")\n",
    "    predicted_labels = predict_naive_bayes(test_x, prior_prob, conditional_prob)\n",
    "\n",
    "    accuracy_value = compute_accuracy(test_y, predicted_labels)\n",
    "    print(f\"Accuracy = {accuracy_value * 100}%\")\n",
    "    conf_matrix = calculate_confusion_matrix(test_y, predicted_labels)\n",
    "    print(f\"\\nConfusion Matrix: \\n{conf_matrix}\")\n",
    "\n",
    "    return accuracy_value, conf_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value of k (for feature selection) = 60\n",
      "Class name to label mapping : {'comp.graphics': 0, 'sci.med': 1, 'talk.politics.misc': 2, 'rec.sport.hockey': 3, 'sci.space': 4}\n",
      "\n",
      "\n",
      "-------------Training set size = 50.0% total data---------------------\n",
      "\n",
      ">> Processing Data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:37<00:00,  7.54s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Computing term ICFs\n",
      ">> Performing feature selection\n",
      ">> Featurizing dataset\n",
      ">> Training Naive Bayes Model\n",
      ">> Testing Naive Bayes Model\n",
      "Accuracy = 99.11999999999999%\n",
      "\n",
      "Confusion Matrix: \n",
      "[[492.   1.   0.   2.   5.]\n",
      " [  4. 495.   1.   0.   0.]\n",
      " [  0.   5. 493.   1.   1.]\n",
      " [  0.   0.   0. 500.   0.]\n",
      " [  1.   0.   1.   0. 498.]]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAdMAAAF3CAYAAAD+c6FVAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAAwgElEQVR4nO3dd5wU9f3H8dfn4FQUBCmecIdCxNilKChijIiCgooVJBZiUNCfUcAWW1SMJhYsoAZzSFUBsSAWrIiKDQ6FKCJGRNTjaNIRDNzd5/fHDpcTuSLDMnuz72ce82DnO7Mznxk3+7lv2e+YuyMiIiLbLiPqAERERKo6JVMREZGQlExFRERCUjIVEREJSclUREQkJCVTERGRkJRMRUQk9sxsgZl9ZmazzGxGUFbXzN4ws6+Cf/cIys3MBpvZPDP71MxaVXR8JVMREUkX7d29hbsfEaxfD0x29/2AycE6wMnAfsHSGxhS0YGVTEVEJF11BUYFr0cBp5cqH+0JHwF1zKxheQdSMhURkXTgwOtm9rGZ9Q7Kstx9UfB6MZAVvM4Gvi/13vygrEzVt2ek29PGgs81z2E5dm3SMeoQpAqyqANIcWa6Q2XZ+N/8pN2cTT/MD/V9v1ODffuQaI7dLNfdc7fY7Rh3X2hmewJvmNnc0hvd3c1sm+NI2WQqIiJporgo1NuDxLll8txyn4XBv0vNbALQBlhiZg3dfVHQjLs02H0h0LjU23OCsjKpmVdERKLlxeGWCpjZbmZWa/NroCMwG3gB6Bns1hOYGLx+AbgwGNV7FLC6VHPwVqlmKiIi0SquOCGGlAVMCJrxqwNj3P1VM8sDxptZL+BboFuw/ySgMzAPWA9cVNEJlExFRCTW3H0+0Hwr5cuBDlspd+DyX3MOJVMREYmUV6KpNtUpmYqISLSS38ybdEqmIiISrRjUTDWaV0REJCTVTEVEJFohf2eaCpRMRUQkWjFo5lUyFRGRaGkAkoiISDhx+GmMBiCJiIiEpJqpiIhES828IiIiIcWgmVfJVEREoqWfxoiIiIQUg5qpBiCJiIiEpJqpiIhESwOQREREQopBM6+SqYiIRCsGNVP1mYqIiISkmqmIiETKXT+NERERCUd9piIiIiHFoM9UyVRERKIVg5qpBiCJiIiEFOuaaVFREedeeh171q/LI/+46WfbChYv5ZZ7HmHF6jXUrlWTf9zUl70a1A91vtVr1nLN7fdRsHgZjfZqwMBbr6F2rZq89MY7DB/3PO7ObrvW4K/9erN/s6ahzpUKhubeR5fOJ7B02Q+0aNkh6nBSTqeOx3H//bdTLSOD4SPGcs+9j0QdUsrIyWnEiOGD2DOrPu7OsMee5KGHh0UdVsr4z5cfsm7djxQVFVFYWEjbo7tEHVJyxWBu3ljXTJ949mWa7p2z1W0DHx3FqR2P47lhD3Dphd0YNPTJSh83b9ZsbrrroV+UDxszgSNbHcbLTzzCka0OY9iY5wDIaZjFiAf/xoThD9LngnMYcN+j23ZBKWb06PF0OeW8qMNISRkZGQwedCennHo+hzZvT/fup3PggftFHVbKKCws5LrrBtC8eXuOOeZULr3sj7o/Wzix4zm0btMp/okUEs28YZYUENtkunjZD0z96GPO6nLCVrfPX5DPka0OBaBNy0OY8v70km0jxj3PuZdey5m9+vPIiHGVPueUD6bTtdNxAHTtdFzJMVsccgC1a9UE4LCDfsuSH5ZvyyWlnKnvTWPFylVRh5GS2rRuyddfL+Cbb75j06ZNjB8/kdNO7RR1WClj8eKlzJw1G4B1635k7tyvaNRor4ijksgUF4dbUkDSkqmZHWBmfzGzwcHyFzM7MFnn29I9Dw+nf58LyciwrW7/7b5NePPdjwCYPHUaP67fwKrVa/kgbxbf5i9i7JB7eGbofcz5z9fM+PfnlTrn8hWraFCvLgD16+7B8hWrfrHPhElvckybltt2UVJlNMrei+/zC0rW8xcuUrIowz775NCi+SFMnz4z6lBShuNMenkMH304iV690qD1JwY106T0mZrZX4AewDhgc5UvBxhrZuPc/a4y3tcb6A3wyN23cvH552zT+d/5cAZ169Tm4P33JS/463dL11zWk78PHsrE16Zw+GEHsWf9umRUy+CDGbP4cMYszrnkagDWb/iJ7/IXcUTzg/nDZX9h46ZNrN/wE6vXruPsi68CoH/vC2i3RYI0M7CfJ/LpMz/juUmTGT3479t0XSJxs9tuuzL+qaFcfc2trF27LupwUkb79mdSULCYBg3q8cqksXz55Tzee29a1GFJOZI1AKkXcLC7bypdaGb3A58DW02m7p4L5AJsLPjct/XkM2fPZcoHeUyd9gn/3biJH9ev5/o7H+Sum/qV7LNn/bo8ePtfAFi/YQNvvPshu9fcDXen1x/OpNtpv2ySGzPkbiDRZ/r8q1O48/orfra9Xt06LFu+ggb16rJs+Qrq7VG7ZNuXXy/g1oH/ZMhdf6VO7VrbemlSRRQsXEzjnEYl6znZDSkoWBxhRKmnevXqjH9qKGPHTuD551+JOpyUsvmzsmzZciZOfJXWrVvEO5mmSFNtGMlq5i0GGm2lvGGwLan6XXI+k59+jNfG/Yt7b7mKNi0P/VkiBVi5eg3FwX/Ax558jjNOToxGbde6Jc+/8hbrN2wAYMmy5SyvZL/gcUe3ZuJrbwMw8bW3aX90GwAWLVlG/1vu4R839KVJ463dFombvBmzaNasKU2aNCYzM5Nu3bry4kuvRx1WShmaex9z587jwUG5UYeSUnbdtQY1a+5W8vqEE47l88+/jDiqJItBn2myaqb9gMlm9hXwfVC2N9AM+HOSzlmhh4eP5eD996V9uzbkzZrNoKFPYgaHH3YQN/XtDcDRrVsw/9t8zrv8BgB2rbELd93Yj3p7VHz8Xj3O5JoBA5kwaTINsxpw362JpuJHR49n1Zq13PFg4kujWrVqPPWve5NzkTvQE48/wu+PbUv9+nVZMH8GA24fyIiRlR+wFWdFRUX07Xczk14eQ7WMDEaOeoo5c/4TdVgpo93RrTn//LP57LM5zMhL/JFx81/v4tVX34o4suhlZTXg6fGPAVC9ejXGjXue119/O9qgkiwOc/Oa+za3ppZ/YLMMoA2QHRQtBPK8knctTDNvOti1SceoQ5AqaOvD8WQzM92hsmz8b37Sbs6Gd0eG+r6vcewfI/8Pl7RJG9y9GPgoWccXEZGYSJGm2jBiPQOSiIhUASny85YwlExFRCRaqpmKiIiEFIOaaWynExQREdlRVDMVEZFoqZlXREQkpBg08yqZiohItGJQM1WfqYiISEiqmYqISLRiUDNVMhURkWipz1RERCQk1UxFRERCikHNVAOQREREQlLNVEREoqVmXhERkZBi0MyrZCoiItFSzVRERCSkGCRTDUASEREJSTVTERGJlnvUEYSmZCoiItGKQTOvkqmIiEQrBslUfaYiIiIhqWYqIiLRisHvTFUzFRGRaBUXh1sqwcyqmdlMM3spWG9qZtPMbJ6ZPWVmOwXlOwfr84LtTSpzfCVTERGJlnu4pXL6Al+UWr8beMDdmwErgV5BeS9gZVD+QLBfhZRMRUQkWkmumZpZDtAFeCxYN+B44Jlgl1HA6cHrrsE6wfYOwf7lUjIVEZG4exC4DticeesBq9y9MFjPB7KD19nA9wDB9tXB/uVK2QFINZt2ijqElLbh+7eiDiFl1Wh8fNQhpKyq/9P45PIYTB5QJYX8aYyZ9QZ6lyrKdffcYNspwFJ3/9jMjgt1onKkbDIVEZE0EXI0b5A4c8vY3A44zcw6A7sAuwODgDpmVj2ofeYAC4P9FwKNgXwzqw7UBpZXFIOaeUVEJFJe7KGWco/tfoO757h7E+Bc4C13Pw+YApwd7NYTmBi8fiFYJ9j+lleiyUI1UxERiVY0MyD9BRhnZncAM4FhQfkw4HEzmwesIJGAK6RkKiIiacHd3wbeDl7PB9psZZ+fgHN+7bGVTEVEJFoxmAFJyVRERKJVQb9nVaBkKiIi0dJTY0REREQ1UxERiVYMaqZKpiIiEq0YzDylZCoiItFSzVRERCSkGIzm1QAkERGRkFQzFRGRaGnSBhERkZBi0MyrZCoiIpFyDUASEREJKQY1Uw1AEhERCUk1UxERiZYGIImIiIQUg2ZeJVMREYlWDAYgqc9UREQkJNVMRUQkWmrmFRERCUkDkEREREJSzVRERCScOMyApAFIIiIiISmZVkJGRgbTp73KhAkjow4laTp2u5gzel7BWX/qS7dLrgp9vImvTKZzjz507tGHia9MBmDDT//lsutu59TzL6PrhZfzwKOjQp8nlQ3NvY+C/H8za+bkqENJSZ06Hsfns99l7pz3uO7ay6MOJ6Wk3b0p9nBLClAyrYQrrujF3Lnzog4j6YYPupNnhw9i/ND7K/2eP155IwsXLflZ2eo1axkychxj/zWQsbn3MWTkOFavXQfAReeezotPDOGZYQ8yc/YXTP3o4+16Dalk9OjxdDnlvKjDSEkZGRkMHnQnp5x6Poc2b0/37qdz4IH7RR1WSkjLe6NkGn/Z2Q05+eQODB8xJupQdrjvFi6izzW30u3i/lz45+uZ/21+pd73/vRPaHtEC2rvXovatWrS9ogWvD/tY2rssjNtWh0GQGZmJgfuty9Llv2QzEuI1NT3prFi5aqow0hJbVq35OuvF/DNN9+xadMmxo+fyGmndoo6rJSQlvfGi8MtKUDJtAL3DbyNG264k+IU+esnWQzoffUtdLu4P0+/8CoAA+59hBv79mH8Yw9wzf9dxB33D6nUsZYsW8Fee9YvWc/asx5Llq342T5r1q7jnQ+mc+ThzbfbNUjV0Sh7L77PLyhZz1+4iEaN9oowotSRlvcmBjXTHT6a18wucvcRZWzrDfQGqFatDhnVdtuhsW2pc+cOLF32AzNnfsaxx7aNNJZkG/3I3WQ1qMfylau45KpbaLp3DrNmz+WqW+8u2Wfjxk0ATJj0Jk888yKQqL1edt3tZGZWJ7thFoPvvLHCcxUWFnHd7QM576xTaBz3LwkRSQtR/DRmALDVZOruuUAuwE4750T+58bRbVtzSpeOnNTpeHbZZWd2370WI0cM5o8XXRl1aNtdVoN6ANTbow4dfncUebNmU6vmbjw7fNAv9j2j8wmc0fkEINFneucNfclumFXqWHXJmzm7ZH3J0uW0bnlIyfptAx9m75xGXNCta7IuR1JcwcLFNM5pVLKek92QgoLFEUaUOtLx3niK1C7DSEozr5l9WsbyGZBV4QFSxM1/vYvf7Nua3+7flvMvuJwpb78fy0S6fsNP/Lh+fcnrD/JmceiB+5HdMIvXprwHgLszd943lTpeuzat+CBvJqvXrmP12nV8kDeTdm1aATB46BOsW7ee66+4ODkXI1VC3oxZNGvWlCZNGpOZmUm3bl158aXXow4rJaTlvVEzb5mygE7Ayi3KDfggSeeUbbR85Sr63vR3AIqKiuh8wu855sjDadI4m7/dP4R/jR5PYWERJ3f4HQc0a1rh8WrvXos+Pbtzbu/ET2wu/eO51N69FouX/kDu4+NpuncO51zcH4AeZ3bh7FM6Ju/iIvTE44/w+2PbUr9+XRbMn8GA2wcyYuS4qMNKCUVFRfTtdzOTXh5DtYwMRo56ijlz/hN1WCkhLe9NDCZtMPftn9XNbBgwwt3f28q2Me7+h4qOkQrNvKnsx+/028Wy1Gh8fNQhiMRO4caFlqxjr/1z51Df97UenpS02CorKTVTd+9VzrYKE6mIiKSRFGmqDUNz84qISLSUTEVERMJJRnfjjqZkKiIi0YpBzVQzIImIiISkmqmIiEQrBjVTJVMREYlUHGZAUjIVEZFoKZmKiIiEVPUnQNIAJBERkbBUMxURkUipz1RERCQsJVMREZGQ1GcqIiIiqpmKiEik1GcqIiISVgyaeZVMRUQkUqqZioiIhBWDmqkGIImIiISkmqmIiETKY1AzVTIVEZFoKZmKiIiEo5qpiIhIWDFIphqAJCIiEpJqpiIiEqk4NPOqZioiIpHy4nBLRcxsFzObbmb/NrPPzWxAUN7UzKaZ2Twze8rMdgrKdw7W5wXbm1R0DiVTERGJVLKTKfBf4Hh3bw60AE4ys6OAu4EH3L0ZsBLoFezfC1gZlD8Q7FcuJVMREYk1T1gXrGYGiwPHA88E5aOA04PXXYN1gu0dzMzKO0fK9pkWe9WfqzGZdtu7Q9QhpKwN374ZdQgpq8Y+J0Qdgsgvebl5qkJm1hvoXaoo191zt9inGvAx0Ax4BPgaWOXuhcEu+UB28Dob+B7A3QvNbDVQD/ihrBhSNpmKiEh6CDsAKUicuRXsUwS0MLM6wATggHBn/TklUxERiZQXh6uZ/qpzua8ysylAW6COmVUPaqc5wMJgt4VAYyDfzKoDtYHl5R1XfaYiIhKpHTCat0FQI8XMagAnAl8AU4Czg916AhOD1y8E6wTb33Ivv+9RNVMREYm7hsCooN80Axjv7i+Z2RxgnJndAcwEhgX7DwMeN7N5wArg3IpOoGQqIiKR8pADkCo+vn8KtNxK+XygzVbKfwLO+TXnUDIVEZFIxWEGJCVTERGJ1I4cgJQsGoAkIiISkmqmIiISqTjM0aNkKiIikYpDM2+ZydTMHiIxd+FWufuVSYlIRETSSqyTKTBjh0UhIiJpK9bNvO4+qvS6me3q7uuTH5KIiEjVUuFoXjNrG8wSMTdYb25m/0x6ZCIikha82EItqaAyP415EOhEMMmvu/8bODaJMYmISBpxt1BLKqjUaF53/36L56IWJSccERFJN+kyA9L3ZnY04GaWCfQlMdu+iIhIaMUpUrsMozLNvJcCl5N48ngB0CJYFxERESpRM3X3H4DzdkAsIiKShlKl3zOMyozm/Y2ZvWhmy8xsqZlNNLPf7IjgREQk/tJlNO8YYDyJh6s2Ap4GxiYzKBERSR/u4ZZUUJlkuqu7P+7uhcHyBLBLsgMTERGpKsqbm7du8PIVM7seGEdirt7uwKQdEJuIiKSBVGmqDaO8AUgfk0iem6+yT6ltDtyQrKBERCR9xOGnMeXNzdt0RwYiIiLpKQ6jeSs1A5KZHQIcRKm+UncfnaygREQkfaTKIKIwKkymZnYrcByJZDoJOBl4D1AyFRERoXI107OB5sBMd7/IzLKAJ5IbVuro1PE47r//dqplZDB8xFjuufeRqENKKf/58kPWrfuRoqIiCgsLaXt0l6hD2i6Kioro3uca9qxfj3/edfPPthUsXspf73mIFavWULtWTe66qT977Vk/1PlWr1nL1QMGUrB4KY322pP7bruW2rVq8tIb7zBs7HPgzq671uCv/S/lgGZVvwdmaO59dOl8AkuX/UCLlh2iDiflpNv9iUOfaWV+GrPB3YuBQjPbHVgKNE5uWKkhIyODwYPu5JRTz+fQ5u3p3v10Djxwv6jDSjkndjyH1m06xSaRAjzx7Ev8Zp+crW4bOGQkp3Vsz4Thg7isZ3ceHPp4pY87feZn3PSPQb8of2zMsxzV6jAmPTmEo1odxrAxzwKQ3TCLkYPuZMKIwVx6YTcG3BePpx+OHj2eLqdoYrWypNv9icNTYyqTTGeYWR1gKIkRvp8AHyYzqFTRpnVLvv56Ad988x2bNm1i/PiJnHZqp6jDkiRbvPQH3v1oBmd1OXGr27/+9nvatDoUgDYtD2XK+9NLtg0fN4Hufa7hjD/15eERlZ/bZMr70+l6UnsAup7UnrfemwZAy0MOoHatmgAcdtD+LFm2fJuuKdVMfW8aK1auijqMlJVu9yctJm1w9/9z91Xu/ihwItDT3S+q6H1mdoCZdTCzmluUn7Tt4e5YjbL34vv8gpL1/IWLaNRorwgjSj2OM+nlMXz04SR69YrHX9J3PzyMq/r0ZIvHDpbYf98mvPnuRwC8OfUjfly/gVWr1/B+3ky+yy9g3KP38uxjDzDny6+Z8e/PK3XO5StW0aBe4qfd9evuwfIVq36xz3Mvv8kxbVpt20WJpLBit1BLKihv0oYy/19rZq3c/ZNytl9J4skyXwDDzKyvu08MNv8deHUb45UU0779mRQULKZBg3q8MmksX345j/eCWlVV9PYHedTdozYH79+M6TM/2+o+11x2EXcOymXiq29xePODyapfj4yMDD7Im8UHebM4++L+AKzf8BPf5hdwRPOD6XHZtWzcuIn1G35i9dp1nNWrHwBX9elJuzYtf3Z8M/tFIp8+8zOem/Qmjz/09+1/0SISWnkDkO4rZ5sDx5ez/RLgcHdfZ2ZNgGfMrIm7D+J/k0D8gpn1BnoDWLXaZGTsVs4pkq9g4WIa5zQqWc/JbkhBweIII0o9m+/HsmXLmTjxVVq3blGlk+nM2XN5+/08pn70Mf/duIkf16/nL3c8wN039y/ZZ8/6dRn0t+sBWL9+A2++8yG716oJOBefdzbdTvtlV8DYIfcCiaQ48dW3uPOGvj/bXq9uHZYtX0GDenVZtnwFdfeoXbLty68XcMu9D/Po3bdQp/buSbhqkWilSr9nGGU287p7+3KW8hIpQIa7rwuOs4DET2tONrP7KSeZunuuux/h7kdEnUgB8mbMolmzpjRp0pjMzEy6devKiy+9HnVYKWPXXWtQs+ZuJa9POOFYPv/8y4ijCqd/7wuY/MwwXn9qKPfecjVtWh72s0QKsHLVGoqLiwEYOuZZzuicGG15dOuWTHjlTdav3wDAkmXLWV7Jfq/jjm7DxFenADDx1Sm0b9cGgEVLltHvr3fxjxv706Rx9va4RJGUE+tm3pCWmFkLd58FENRQTwGGA4cm6ZzbXVFREX373cykl8dQLSODkaOeYs6c/0QdVsrIymrA0+MfA6B69WqMG/c8r7/+drRBJcnDw8dw8P7NaN+uDXmzZvPg0McxMw4/7CBu7peYabNd65bM/zaf8y7/CwC71qjBP27qR7096lR4/Iv/cCZXD7iX5ya9SaOsBtx327UADBn1FKvXrOWOBx4FoFq1aozPLa/RqGp44vFH+P2xbalfvy4L5s9gwO0DGTFyXNRhpYx0uz8pMoYoFPMkDIUysxyg0N1/0SZqZu3c/f2KjlF9p+w43N+kyShjcIzAjwveiDqElFVjnxOiDkGqqMKNC5P2pfNRozNDfd8fVfBc5F+ISamZunt+OdsqTKQiIpI+UqWpNowKfxpjCeeb2S3B+t5m1ib5oYmISDpIl0kb/gm0BXoE62sBzaknIiLbRXHIJRVUppn3SHdvZWYzAdx9pZntlOS4REQkTXjZP/KoMipTM91kZtUIBlyZWQNS548BERGRyFWmZjoYmADsaWZ3kniKzM3lv0VERKRyimPw240Kk6m7P2lmHwMdSEy4cLq7f5H0yEREJC0Ux6CZtzIPB98bWA+8WLrM3b9LZmAiIpIe4tBnWplm3pdJ9JcasAvQFPgSODiJcYmISJqIwyCcyjTz/mz6v+BpMv+XtIhERESqmF89A5K7f2JmRyYjGBERST9p0cxrZleVWs0AWgEFZewuIiLyq6RFMy9Qq9TrQhJ9qM8mJxwREUk3sU+mwWQNtdz9mh0Uj4iISJVTZjI1s+ruXmhm7XZkQCIikl7i3mc6nUT/6CwzewF4Gvhx80Z3fy7JsYmISBoorvq5tFJ9prsAy4Hj+d/vTR1QMhURkdDiPgPSnsFI3tn8L4luFoOZFEVEJBXEIaGUl0yrATVhq38yxOHaRUREtovykukid799h0UiIiJpKe4/jan6jdgiIpLyiq3qp5vykmmHHRaFiIikrTj0G5aZTN19xY4MRERE0lMcmnkzog5ARESkqvvVT40RERHZnuIwaYNqpiIiEqliLNRSETNrbGZTzGyOmX1uZn2D8rpm9oaZfRX8u0dQbmY22MzmmdmnwXO8y6VkKiIikfKQSyUUAle7+0HAUcDlZnYQcD0w2d33AyYH6wAnA/sFS29gSEUnUDIVEZFYc/dF7v5J8Hot8AWQDXQFRgW7jQJOD153BUZ7wkdAHTNrWN451GdaRRV7HAaTJ0eNfU6IOoSUtaFgatQhpLQajX4XdQhpKWyfqZn1JlGD3CzX3XPL2LcJ0BKYBmS5+6Jg02IgK3idDXxf6m35QdkiyqBkKiIikQr705ggcW41eZZmZjWBZ4F+7r7GSk0W4e5uZttcS1Ezr4iIRGoH9JliZpkkEumTpR4humRz823w79KgfCHQuNTbc4KyMimZiohIpIot3FIRS1RBhwFfuPv9pTa9APQMXvcEJpYqvzAY1XsUsLpUc/BWqZlXRETirh1wAfCZmc0Kym4E7gLGm1kv4FugW7BtEtAZmAesBy6q6ARKpiIiEqlkTyfo7u9R9sNbfjEPvbs7cPmvOYeSqYiIRCoOc/MqmYqISKQ8BtMJKpmKiEik4lAz1WheERGRkFQzFRGRSMWhZqpkKiIikYrD5KhKpiIiEik9z1RERERUMxURkWipz1RERCQkJVMREZGQNABJREQkJA1AEhEREdVMRUQkWuozFRERCUl9piIiIiEVxyCdqs9UREQkJNVMRUQkUuozFRERCanqN/IqmYqISMRUMxUREQlJkzaIiIiIaqYiIhIt/TQmDXTqeByfz36XuXPe47prL486nJSj+1O2dLk3Hc/qyRkXXMZZPS+n25+uDH28iZPeoHP3XnTu3ouJk94AYMNPP3HZNbdwao9L6HpeHx4YMjz0eVJZunx2NvOQSypQzbQcGRkZDB50Jyd17kF+/iI++nASL770Ol988VXUoaUE3Z+ypdu9Gf7QXexRp/aves8f/3wdd950NdkNs0rKVq9Zy5ARY3hq2GAAuve6kuOOOYqddsrkoh5n0ebw5mzatIleV97A1A/z+F3b1tv1OlJBun12IB4DkFQzLUeb1i35+usFfPPNd2zatInx4ydy2qmdog4rZej+lC3d7813+QX0uepmuv3pCi687Brmf/t9pd73/rSPadu6JbV3r0Xt3WvRtnVL3p/2MTV22YU2hzcHIDMzkwP3b8aSZT8k8xIik46fnWI81JIKkpZMzayNmbUOXh9kZleZWedknS8ZGmXvxff5BSXr+QsX0ajRXhFGlFp0f8qWTvfGzOjd/ya6/ekKnp44CYAB9wzmxv6XMX74Q1zz54u5Y+AjlTrWkmU/sNeeDUrWsxrU/0XSXLN2He+8P40jD2+x3a4hlaTTZydOktLMa2a3AicD1c3sDeBIYApwvZm1dPc7y3hfb6A3gFWrTUbGbskIT0S2o9FDBpLVoD7LV67ikn430nSfxsz67AuuuvnvJfts3LQJgAkvv84T4ycC8N3CAi675q9kVs8ku1EWg/9xS4XnKiws4rrb7ua8s0+jcXbD5FyQ7HCpUbcMJ1l9pmcDLYCdgcVAjruvMbOBwDRgq8nU3XOBXIDqO2VHfn8LFi6mcU6jkvWc7IYUFCyOMKLUovtTtnS6N1kN6gNQb486dDj2aPI++ZRatXbj2VG/rI2e0aUjZ3TpCGy9zzSrQX3yZn5asr5k2Q+0bnlYyfpt9wxi75xGXND9jGRdTuTS6bOzmfpMy1bo7kXuvh742t3XALj7BqrQfcubMYtmzZrSpEljMjMz6datKy++9HrUYaUM3Z+ypcu9Wb/hJ378cX3J6w+mf8KhB+1PdsO9eO2tqQC4O3O/ml+p47U78nA+mP4Jq9esZfWatXww/RPaHXk4AINzR7Fu3Xqu79snOReTItLls1NaHPpMk1Uz3WhmuwbJ9PDNhWZWmyqUTIuKiujb72YmvTyGahkZjBz1FHPm/CfqsFKG7k/Z0uXeLF+xkr43/g2AosIiOnc8jmOOOoIme+fwt4EP869RYyksLOTkDr/ngP1+U+Hxau9eiz5/7MG5F/cF4NKL/kDt3WuxeOkyckeNo+k+jTnnoisA6HHWqZx92knJu7iIpMtnJ27MfftndTPb2d3/u5Xy+kBDd/+somOkQjOvSNxsKJgadQgprUaj30UdQsoq3LgwaZP+9W9ybqjv+wcWjIt8QsKk1Ey3lkiD8h+AeI5nFxGRbVJlmivLoUkbREQkUp4i/Z5hKJmKiEik4lAz1QxIIiIiIalmKiIikUqVn7eEoWQqIiKRqvqpVMlUREQippqpiIhISBqAJCIiIqqZiohItPQ7UxERkZDi0MyrZCoiIpGKQ81UfaYiIiIhqWYqIiKRUjOviIhISMVJeBTojqZkKiIikar6qVTJVEREIhaHGZA0AElERCQk1UxFRCRScfhpjJKpiIhESqN5RUREQopDn6mSqYiIRCoOzbwagCQiIhKSaqYiIhKpOPSZqmYqIiKRcvdQS0XMbLiZLTWz2aXK6prZG2b2VfDvHkG5mdlgM5tnZp+aWavKXIOSqYiIRKoYD7VUwkjgpC3Krgcmu/t+wORgHeBkYL9g6Q0MqcwJlExFRCTW3P1dYMUWxV2BUcHrUcDppcpHe8JHQB0za1jROdRnKiIikYqozzTL3RcFrxcDWcHrbOD7UvvlB2WLKIeSqUgaqdHod1GHkNI25L8ddQhpKexPY8ysN4km2c1y3T230ud3dzMLFYSSqYiIRCrspA1B4qx08gwsMbOG7r4oaMZdGpQvBBqX2i8nKCuX+kxFRCRSyR7NW4YXgJ7B657AxFLlFwajeo8CVpdqDi6TaqYiIhJrZjYWOA6ob2b5wK3AXcB4M+sFfAt0C3afBHQG5gHrgYsqcw4lUxERiVSyByC5e48yNnXYyr4OXP5rz6FkKiIikYrD3LxKpiIiEik9NUZERCSkEIOIUoZG84qIiISkmqmIiERKzbwiIiIhaQCSiIhISMXqMxURERHVTEVEJFJVv16qZCoiIhHTACQREZGQlExFRERC0qQNIiIiopqpiIhES828IiIiIWnSBhERkZDi0GeqZCoiIpGKQzOvBiCJiIiEpJqpiIhESs28IiIiIcWhmVfJVEREIhWH0bzqMxUREQlJNVMREYmUnmeaBobm3kdB/r+ZNXNy1KGkpE4dj+Pz2e8yd857XHft5VGHk1L02Slfunx2Op7zJ87oeTlnXXQF3S7uF/p4E1+ZTOcel9C5xyVMfCXx2drw009cdu1tnHrepXS94P944NGRoc+zI3nI/6UCJdMKjB49ni6nnBd1GCkpIyODwYPu5JRTz+fQ5u3p3v10Djxwv6jDShn67JQt3T47wwf9nWdHPMT4xx6s9Hv+eMX1LFy05Gdlq9esZciIMYz91/2MzX2AISPGsHrtOgAu6nEmLz75KM8MH8TMz+Yw9aMZ2/MSkqrYPdSSCpRMKzD1vWmsWLkq6jBSUpvWLfn66wV88813bNq0ifHjJ3LaqZ2iDitl6LNTtnT/7Hy3cBF9rr6Fbr36cuHl1zH/2+8r9b73p39C29Ytqb17LWrXqknb1i15f9rH1NhlF9q0OgyAzMxMDvztvixZ+kMyL2G7Us30VzCz0TvqXLJjNMrei+/zC0rW8xcuolGjvSKMSKqKdPrsmBm9r0okzqdfeBWAAfc8xI39+jB+2CCu+b9e3HH/kEoda8my5ey1Z/2S9awG9ViybPnP9lmzdh3vvD+dI49osd2uQSqWlAFIZvbClkVAezOrA+Dup5Xxvt5AbwCrVpuMjN2SEZ6IyA4z+pG7yWpQn+UrV3FJ/5tpuncOs2bP5apb7irZZ+OmTQBMePkNnngm8fX53cJFXHbtbWRmVie7YRaD/35zhecqLCziugH3ct7Zp9G4Cv1xkipNtWEkazRvDjAHeAxwEsn0COC+8t7k7rlALkD1nbKr/t2NuYKFi2mc06hkPSe7IQUFiyOMSKqKdPrsZDVI1CTr7VGHDse2JW/mZ9SquRvPjnjoF/ue0eVEzuhyIpDoM73zxv5kN8wqdax65M38rGR9ybLltG55aMn6bfc+xN45jbigW9dkXU5SpEpTbRjJauY9AvgYuAlY7e5vAxvc/R13fydJ55QdLG/GLJo1a0qTJo3JzMykW7euvPjS61GHJVVAunx21m/4iR/Xry95/UHeTA496LdkN8ritSnvAYmp9ObOm1+p47Vr04oP8mayeu06Vq9dxwd5M2nXphUAg4c+zrof13P9lZck52KSKA4DkJJSM3X3YuABM3s6+HdJss6VbE88/gi/P7Yt9evXZcH8GQy4fSAjRo6LOqyUUFRURN9+NzPp5TFUy8hg5KinmDPnP1GHlTL02Slbunx2lq9cRd8b7wCgqKiYzif+nmOOPJwmjbP5233/5F+jxlFYWMTJHY7lgGa/qfB4tXevRZ+e3Tn3kv4AXNrzXGrvXovFS38gd/RTNN0nh3N69QWgx5mncHYVGdQVh5qp7YgJhs2sC9DO3W+s7HvUzCsiO9qG/LejDiFlZe65nyXr2L+p3zLU9/38H2YmLbbK2iG1RXd/GXh5R5xLRESqlkRjZtVWJZteRUQkPvTUGBERkZDi8DxTzYAkIiISkmqmIiISKTXzioiIhBSHZl4lUxERiVSqTLwQhpKpiIhEKg6TNmgAkoiISEiqmYqISKTUZyoiIhKSRvOKiIiEFIeaqfpMRUREQlLNVEREIqWfxoiIiIQUh2ZeJVMREYmUBiCJiIiEFIeaqQYgiYiIhKSaqYiIREoDkEREREKKw9y8SqYiIhIp1UxFRERC0gAkERERUc1URESiFYc+U9VMRUQkUu4eaqkMMzvJzL40s3lmdv32vgbVTEVEJFLJ7jM1s2rAI8CJQD6QZ2YvuPuc7XUO1UxFRCTu2gDz3H2+u28ExgFdt+cJlExFRCRSHnKphGzg+1Lr+UHZdpOyzbyFGxda1DGUZma93T036jhSke5N+XR/yqZ7U750uT9hv+/NrDfQu1RR7o6+b6qZVl7vindJW7o35dP9KZvuTfl0fyrB3XPd/YhSy5aJdCHQuNR6TlC23SiZiohI3OUB+5lZUzPbCTgXeGF7niBlm3lFRES2B3cvNLM/A68B1YDh7v759jyHkmnlxb7fIgTdm/Lp/pRN96Z8uj/bibtPAiYl6/gWhzkRRUREoqQ+UxERkZCUTCuQ7CmoqjIzG25mS81sdtSxpBoza2xmU8xsjpl9bmZ9o44plZjZLmY23cz+HdyfAVHHlGrMrJqZzTSzl6KORSqmZFqOUlNQnQwcBPQws4OijSqljAROijqIFFUIXO3uBwFHAZfrs/Mz/wWOd/fmQAvgJDM7KtqQUk5f4Iuog5DKUTItX9KnoKrK3P1dYEXUcaQid1/k7p8Er9eS+FLcrjOuVGWesC5YzQwWDeAImFkO0AV4LOpYpHKUTMuX9CmoJP7MrAnQEpgWcSgpJWjGnAUsBd5wd92f/3kQuA4ojjgOqSQlU5EkMrOawLNAP3dfE3U8qcTdi9y9BYnZaNqY2SERh5QSzOwUYKm7fxx1LFJ5SqblS/oUVBJfZpZJIpE+6e7PRR1PqnL3VcAU1P++WTvgNDNbQKJr6XgzeyLakKQiSqblS/oUVBJPZmbAMOALd78/6nhSjZk1MLM6wesaJJ4zOTfSoFKEu9/g7jnu3oTEd85b7n5+xGFJBZRMy+HuhcDmKai+AMZv7ymoqjIzGwt8COxvZvlm1ivqmFJIO+ACErWKWcHSOeqgUkhDYIqZfUrij9Y33F0/AZEqSzMgiYiIhKSaqYiISEhKpiIiIiEpmYqIiISkZCoiIhKSkqmIiEhISqYSO2ZWFPwUZbaZPW1mu4Y41kgzOzt4/Vh5k9Wb2XFmdvQ2nGOBmdWvbPkW+6wrb/tW9r/NzK75tTGKSPmUTCWONrh7C3c/BNgIXFp6o5lV35aDuvvF7j6nnF2OA351MhWRqk/JVOJuKtAsqDVONbMXgDnBJOv3mlmemX1qZn0gMXORmT0cPMP2TWDPzQcys7fN7Ijg9Ulm9knwPM7JwWT2lwL9g1rx74JZfp4NzpFnZu2C99Yzs9eD53g+BlhFF2Fmz5vZx8F7em+x7YGgfLKZNQjK9jWzV4P3TDWzA7bL3RSRrdqmv9BFqoKgBnoy8GpQ1Ao4xN2/CRLSandvbWY7A++b2esknu6yP4nn12YBc4DhWxy3ATAUODY4Vl13X2FmjwLr3H1gsN8Y4AF3f8/M9iYxk9aBwK3Ae+5+u5l1ASozc9SfgnPUAPLM7Fl3Xw7sBsxw9/5mdktw7D8DucCl7v6VmR0J/BM4fhtuo4hUgpKpxFGN4NFekKiZDiPR/Drd3b8JyjsCh23uDwVqA/sBxwJj3b0IKDCzt7Zy/KOAdzcfy93LeqbrCcBBiWl6Adg9eIrMscCZwXtfNrOVlbimK83sjOB14yDW5SQe0fVUUP4E8FxwjqOBp0ude+dKnENEtpGSqcTRhuDRXiWCpPJj6SLgCnd/bYv9tuf8uRnAUe7+01ZiqTQzO45EYm7r7uvN7G1glzJ29+C8q7a8ByKSPOozlXT1GnBZ8Jg0zOy3ZrYb8C7QPehTbQi038p7PwKONbOmwXvrBuVrgVql9nsduGLzipm1CF6+C/whKDsZ2KOCWGsDK4NEegCJmvFmGcDm2vUfSDQfrwG+MbNzgnOYmTWv4BwiEoKSqaSrx0j0h35iZrOBf5FoqZkAfBVsG03iqTg/4+7LgN4kmlT/zf+aWV8Eztg8AAm4EjgiGOA0h/+NKh5AIhl/TqK597sKYn0VqG5mXwB3kUjmm/1I4sHas0n0id4elJ8H9Ari+xzoWol7IiLbSE+NERERCUk1UxERkZCUTEVEREJSMhUREQlJyVRERCQkJVMREZGQlExFRERCUjIVEREJSclUREQkpP8HeSoi5KEllp8AAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 576x432 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---------------------------------------------------------------\n",
      "\n",
      "-------------Training set size = 70.0% total data---------------------\n",
      "\n",
      ">> Processing Data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 4/5 [00:30<00:07,  7.64s/it]"
     ]
    }
   ],
   "source": [
    "\n",
    "train_ratios = [0.5, 0.7, 0.8]\n",
    "k = int(input(\"Enter the value of k : \"))\n",
    "print(f\"Value of k (for feature selection) = {k}\")\n",
    "print(f\"Class name to label mapping : {class_name_to_label}\\n\")\n",
    "for i in range(len(train_ratios)):\n",
    "    # random.seed()\n",
    "    print(f\"\\n-------------Training set size = {train_ratios[i] * 100}% total data---------------------\\n\")\n",
    "    accuracy, conf_matrix = run_question3(train_ratios[i], k)\n",
    "    plot_confusion_matrix(conf_matrix, f\"conf_matrix_k_{k}_train_{i}.png\", save=True)\n",
    "    print(\"\\n---------------------------------------------------------------\")\n",
    "     "
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2370e07001db70a9c24f7e21173c51fbc4321340913a02830aed4885459fa0a0"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
