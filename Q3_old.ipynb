{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\Samyak\n",
      "[nltk_data]     Jain\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to C:\\Users\\Samyak\n",
      "[nltk_data]     Jain\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to C:\\Users\\Samyak\n",
      "[nltk_data]     Jain\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to C:\\Users\\Samyak\n",
      "[nltk_data]     Jain\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import copy\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import contractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = [\"comp.graphics\", \"sci.med\", \"talk.politics.misc\", \"rec.sport.hockey\", \"sci.space\"]\n",
    "class_labels = [0, 1, 2, 3, 4]\n",
    "class_name_to_label = {class_names[i]:i for i in range(len(class_names))}\n",
    "class_label_to_name = {i:class_names[i] for i in range(len(class_names))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = 'data/20_newsgroups/20_newsgroups'\n",
    "dir_names = class_names #reading the data directory to list all the files\n",
    "# class_wise_doc_id_tuples\n",
    "docID_to_doc_mapping = {}\n",
    "class_wise_file_paths = {}\n",
    "num_docs_per_class = {}\n",
    "for i in range(len(dir_names)):\n",
    "    class_data_dir = data_dir + \"/\" + dir_names[i]\n",
    "    file_names = os.listdir(class_data_dir)\n",
    "    file_paths = [(class_data_dir + '/' + fname) for fname in file_names]\n",
    "    class_wise_file_paths[i] = file_paths\n",
    "    num_docs_per_class[i] = len(file_names)\n",
    "    for j in range(len(file_names)):\n",
    "        docID_to_doc_mapping[i, j] = file_names[j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_alnum(tok):\n",
    "    '''\n",
    "        Remove non-alphanumeric characters from a string\n",
    "    '''\n",
    "\n",
    "    tok = ''.join(ch for ch in tok if ch.isalnum() == True)\n",
    "    return tok\n",
    "\n",
    "def remove_punct(tok):\n",
    "    '''\n",
    "        Remove the punctuation in token\n",
    "    '''\n",
    "    punctuations = string.punctuation\n",
    "    tok = ''.join(ch for ch in tok if ch not in punctuations)\n",
    "    return tok\n",
    "\n",
    "def remove_blank_space(tok):\n",
    "    '''\n",
    "        Remove the spaces in token\n",
    "    '''\n",
    "    tok = ''.join(ch for ch in tok if ch != ' ')\n",
    "    return tok\n",
    "def preprocess(text):\n",
    "\n",
    "    text = text.lower()\n",
    "\n",
    "    text = contractions.fix(text)\n",
    "\n",
    "    all_tokens = word_tokenize(text)\n",
    "\n",
    "    all_tokens = [check_alnum(tok) for tok in all_tokens]\n",
    "\n",
    "    stop_words = list(set(stopwords.words('english')))\n",
    "\n",
    "    all_tokens = [tok for tok in all_tokens if tok not in stop_words]\n",
    "\n",
    "    toks_no_punct = []\n",
    "    for tok in all_tokens:\n",
    "        ctok = remove_punct(tok)\n",
    "        if(ctok != \"\"):\n",
    "            toks_no_punct.append(ctok)\n",
    "\n",
    "    cleaned_toks = []\n",
    "    for tok in toks_no_punct:\n",
    "        ctok = remove_blank_space(tok)\n",
    "        if(ctok != \"\"):\n",
    "            cleaned_toks.append(ctok)\n",
    "    \n",
    "    final_tokens = [tok for tok in cleaned_toks]\n",
    "\n",
    "    return final_tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def readClassFiles(class_file_paths):\n",
    "#     class_doc_tokens = []\n",
    "#     for fpath in class_file_paths:\n",
    "#         f = open(fpath, 'r', encoding='utf-8', errors='ignore')\n",
    "#         ftxt_unprocessed = f.read()\n",
    "#         doc_toks = preprocess(ftxt_unprocessed)\n",
    "#         class_doc_tokens.append(doc_toks)\n",
    "#     return class_doc_tokens\n",
    "\n",
    "\n",
    "# class_doc_wise_tokens = {}\n",
    "# class_wise_tokens = {}\n",
    "# for label in class_labels:\n",
    "#     class_doc_wise_tokens[label] = readClassFiles(class_wise_file_paths[label])\n",
    "#     class_toks = set()\n",
    "#     for doc_toks in class_doc_wise_tokens[label]:\n",
    "#         for tok in doc_toks:\n",
    "#             class_toks.add(tok)\n",
    "#     class_wise_tokens[label] = class_toks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2370e07001db70a9c24f7e21173c51fbc4321340913a02830aed4885459fa0a0"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
