{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\Samyak\n",
      "[nltk_data]     Jain\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to C:\\Users\\Samyak\n",
      "[nltk_data]     Jain\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to C:\\Users\\Samyak\n",
      "[nltk_data]     Jain\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to C:\\Users\\Samyak\n",
      "[nltk_data]     Jain\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import copy\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = 'data/Humor,Hist,Media,Food'\n",
    "file_names = os.listdir(data_dir) #reading the data directory to list all the files\n",
    "file_paths = [(data_dir + '/' + fname) for fname in file_names] #forming file paths\n",
    "docID_to_doc_mapping = {} #forming docID to doc name mapping\n",
    "for i in range(len(file_names)):\n",
    "    docID_to_doc_mapping[i] = file_names[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punct(tok):\n",
    "    '''\n",
    "        Removing punctations from tokens\n",
    "    '''\n",
    "    punctuations = string.punctuation\n",
    "    tok = ''.join(x for x in tok if x not in punctuations)\n",
    "    return tok\n",
    "def remove_space(tok):\n",
    "    '''\n",
    "        Removing blank space toks\n",
    "    '''\n",
    "    tok = ''.join(x for x in tok if x != ' ')\n",
    "    return tok\n",
    "\n",
    "def preprocess_file(file_text):\n",
    "    '''\n",
    "        This function preprocesses the file text.\n",
    "        Input: file_text in string form represting the text of a file\n",
    "        Returns: cleaned_toks, word tokens present in the file after preprocessing\n",
    "    '''\n",
    "\n",
    "    #converting the text to lowercase\n",
    "    ftext = file_text.lower()\n",
    "\n",
    "    #performing word tokenization\n",
    "    file_toks = word_tokenize(ftext)\n",
    "\n",
    "    #removing the stopwords from tokens\n",
    "    stop_words = list(set(stopwords.words('english')))\n",
    "    file_toks = [tok for tok in file_toks if tok not in stop_words]\n",
    "\n",
    "    #removing punctuation marks from tokens\n",
    "    toks_no_punct = []\n",
    "    for tok in file_toks:\n",
    "        ctok = remove_punct(tok)\n",
    "        if(ctok != \"\"):\n",
    "            toks_no_punct.append(ctok)\n",
    "    \n",
    "    #Removing blank space tokens\n",
    "    cleaned_toks = []\n",
    "    for tok in toks_no_punct:\n",
    "        ctok = remove_space(tok)\n",
    "        if(ctok != \"\"):\n",
    "            cleaned_toks.append(ctok)\n",
    "\n",
    "    return cleaned_toks\n",
    "\n",
    "def cleanQuery(query_text):\n",
    "    '''\n",
    "        Preprocessing the query text\n",
    "        Input: query_text, string of the phrase query text\n",
    "        Returns: cleaned_toks, an array containg the preprocessed query tokens\n",
    "    '''\n",
    "\n",
    "    #We perform the same preprocessing steps on the query as we did for the file text\n",
    "\n",
    "    #converting the text to lowercase\n",
    "    qtext = query_text.lower()\n",
    "    \n",
    "    #performing word tokenization\n",
    "    query_toks = word_tokenize(qtext)\n",
    "    \n",
    "    #removing the stopwords from tokens\n",
    "    stop_words = list(set(stopwords.words('english')))\n",
    "    query_toks = [tok for tok in query_toks if tok not in stop_words]\n",
    "    \n",
    "    #removing punctuation marks from tokens\n",
    "    toks_no_punct = []\n",
    "    for tok in query_toks:\n",
    "        ctok = remove_punct(tok)\n",
    "        if(ctok != \"\"):\n",
    "            toks_no_punct.append(ctok)\n",
    "    \n",
    "    #Removing blank space tokens\n",
    "    cleaned_toks = []\n",
    "    for tok in toks_no_punct:\n",
    "        ctok = remove_space(tok)\n",
    "        if(ctok != \"\"):\n",
    "            cleaned_toks.append(ctok)\n",
    "    \n",
    "    return cleaned_toks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(fpaths):\n",
    "    '''\n",
    "        Reads the files and preprocess every file's text to form word tokens for every file.\n",
    "        Returns a 2-D list containing word tokens for every file\n",
    "    '''\n",
    "    file_tokens = []\n",
    "    for fpath in fpaths:\n",
    "        f = open(fpath, 'r', encoding='utf-8', errors='ignore') #open the file\n",
    "        ftxt_unprocessed = f.read() #read the text of the file\n",
    "        ftoks = preprocess_file(ftxt_unprocessed) #preprocessing the text to form word tokens\n",
    "        file_tokens.append(ftoks)\n",
    "    return file_tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDocsFromID(docID_to_doc, doc_IDs):\n",
    "    '''\n",
    "        Given a list of document IDs, it outputs the document names corresponding to thos IDs.\n",
    "        Input: docID_to_docs (mapping between docID -> doc_name), docIDs - list of input document IDs\n",
    "        Returns: doc_names - list of doc_names corresponding to document IDs in doc_IDs\n",
    "    '''\n",
    "    doc_names = []\n",
    "    for doc_ID in doc_IDs:\n",
    "        doc_names.append(docID_to_doc[doc_ID])\n",
    "    return doc_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_toks = read_file(file_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary_set = set()\n",
    "for doc_tok in document_toks:\n",
    "    for tok in doc_tok:\n",
    "        vocabulary_set.add(tok)\n",
    "vocabulary_list = list(vocabulary_set)\n",
    "id_to_term = {}\n",
    "term_to_id = {}\n",
    "for i in range(len(vocabulary_list)):\n",
    "    id_to_term[i] = vocabulary_list[i]\n",
    "    term_to_id[vocabulary_list[i]] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_jaccard_coeff(query_toks, doc_toks):\n",
    "    query_tok_set = set(query_toks)\n",
    "    doc_toks_set = set(doc_toks)\n",
    "    num_intersection = len(list(query_tok_set & doc_toks_set))\n",
    "    num_union = len(list(query_tok_set | doc_toks_set))\n",
    "    jaccard = num_intersection / num_union\n",
    "    return jaccard\n",
    "\n",
    "def perform_jaccard_scoring(query_toks, document_toks):\n",
    "    jaccard_coeff_values = {}\n",
    "    for i in range(len(document_toks)):\n",
    "        jaccard_coeff_i = compute_jaccard_coeff(query_toks, document_toks[i])\n",
    "        jaccard_coeff_values[i] = jaccard_coeff_i\n",
    "    return jaccard_coeff_values\n",
    "\n",
    "def get_relevant_by_jaccard(jaccard_coeff_values):\n",
    "    ranked_order_by_jaccard = dict(sorted(jaccard_coeff_values.items(), key=lambda item: item[1], reverse=True))\n",
    "    top_5_docID = list(ranked_order_by_jaccard.keys())[:5]\n",
    "    top_5_jaccard = list(ranked_order_by_jaccard.values())[:5]\n",
    "    return top_5_docID, top_5_jaccard\n",
    "\n",
    "def run_jaccard(query, document_toks):\n",
    "    query_toks = cleanQuery(query)\n",
    "    if(len(query_toks) == 0):\n",
    "        print(\"no. of query tokens after preprocessing is 0. Jaccard coefficient with all documents is equal to 0\")\n",
    "        for i in range(5):\n",
    "            print(f\"{i + 1} : {docID_to_doc_mapping[i]} (0)\") \n",
    "    jaccard_scores = perform_jaccard_scoring(query_toks, document_toks)\n",
    "    # print(jaccard_scores)\n",
    "    top_5_doc_ids, top_5_jaccard_score = get_relevant_by_jaccard(jaccard_scores)\n",
    "    top_5_doc_names = getDocsFromID(docID_to_doc_mapping, top_5_doc_ids)\n",
    "    print(f\"Query Text = {query}\")\n",
    "    print(f\"Query tokens after preprocessing = {query_toks}\")\n",
    "    print(f\"Top 5 relevant documents based on the value of the Jaccard coefficient : \")\n",
    "    for i in range(len(top_5_doc_names)):\n",
    "        print(f\"{i + 1} : {top_5_doc_names[i]} ({top_5_jaccard_score[i]})\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run_jaccard(\"adjpasdps dalsdnal alsndlandl\", document_toks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([1, 2, 3, 4, 5]), array([3, 3, 1, 2, 1], dtype=int64))\n"
     ]
    }
   ],
   "source": [
    "# a = np.array([1, 2, 1, 2, 3, 4, 4, 5 ,1, 2])\n",
    "# print(np.unique(a, return_counts=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_raw_term_frequency(document_toks):\n",
    "\n",
    "    raw_term_freq = {}\n",
    "    for i in range(len(document_toks)):\n",
    "        raw_term_freq[i] = {}\n",
    "        unique_toks, tok_freq = np.unique(document_toks[i], return_counts=True)\n",
    "        for j in range(len(unique_toks)):\n",
    "            raw_term_freq[i][unique_toks[j]] = tok_freq[j]\n",
    "    return raw_term_freq\n",
    "def generate_term_postings(document_toks):\n",
    "\n",
    "    term_posting_lists = {}\n",
    "    for i in range(len(document_toks)):\n",
    "        for tok in document_toks[i]:\n",
    "            if(tok not in term_posting_lists.keys()):\n",
    "                term_posting_lists[tok] = [i]\n",
    "            else:\n",
    "                if(i not in term_posting_lists[tok]):\n",
    "                    term_posting_lists[tok].append(i)\n",
    "    return term_posting_lists\n",
    "\n",
    "def compute_document_frequency(term_posting_lists):\n",
    "\n",
    "    term_df = {}\n",
    "    for term in term_posting_lists.keys():\n",
    "        term_df[term] = len(term_posting_lists[term])\n",
    "    return term_df\n",
    "\n",
    "def compute_IDF(term_df, num_total_docs):\n",
    "\n",
    "    term_idf = {}\n",
    "    for term in term_df.keys():\n",
    "        idf_value = np.log10(num_total_docs / (term_df[term] + 1))\n",
    "        term_idf[term] = idf_value\n",
    "    return term_idf\n",
    "\n",
    "def compute_tf_weight(scheme, term, doc_tfs):\n",
    "\n",
    "    if(scheme == \"binary\"):\n",
    "        if(term in doc_tfs.keys()):\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "    elif(scheme == \"raw_count\"):\n",
    "        if(term in doc_tfs.keys()):\n",
    "            return doc_tfs[term]\n",
    "        else:\n",
    "            return 0\n",
    "    elif(scheme == \"term_frequency\"):\n",
    "        if(term in doc_tfs.keys()):\n",
    "            total_terms = sum(doc_tfs.values())\n",
    "            return doc_tfs[term] / total_terms\n",
    "        else:\n",
    "            return 0\n",
    "    elif(scheme == \"log_normalization\"):\n",
    "        if(term in doc_tfs.keys()):\n",
    "            return np.log10(1 + doc_tfs[term])\n",
    "        else:\n",
    "            return 0\n",
    "    elif(scheme == \"double_normalization\"):\n",
    "        if(term in doc_tfs.keys()):\n",
    "            t1 = 0.5\n",
    "            t2 = (0.5)*(doc_tfs[term] / max(doc_tfs.values()))\n",
    "            return t1 + t2\n",
    "        else:\n",
    "            return 0.5\n",
    "\n",
    "def generate_tf_idf_matrices(document_toks, vocabulary_list):\n",
    "    num_docs = len(list(document_toks))\n",
    "    print(f\"Num docs = {num_docs}\")\n",
    "    raw_tfs = compute_raw_term_frequency(document_toks)\n",
    "    term_wise_postings = generate_term_postings(document_toks)\n",
    "    term_document_freq = compute_document_frequency(term_wise_postings)\n",
    "    term_idfs = compute_IDF(term_document_freq, num_docs)\n",
    "    num_words = len(vocabulary_list)\n",
    "    tf_idf_matrix_by_scheme = {}\n",
    "    schemes_list = ['binary', 'raw_count', 'term_frequency', 'log_normalization', 'double_normalization']\n",
    "    for scheme in schemes_list:\n",
    "        tf_idf_matrix_by_scheme[scheme] = np.zeros((num_docs, num_words))\n",
    "    for scheme in schemes_list:\n",
    "        print(f\"Generating for scheme : {scheme}\")\n",
    "        for i in tqdm(range(num_docs)):\n",
    "            for j in range(num_words):\n",
    "                tf_weight = compute_tf_weight(scheme, id_to_term[j], raw_tfs[i])\n",
    "                idf = term_idfs[id_to_term[j]]\n",
    "                tf_idf_matrix_by_scheme[scheme][i][j] = tf_weight * idf\n",
    "    return raw_tfs, term_idfs, tf_idf_matrix_by_scheme\n",
    "# def generate_tf_idf_matrices(document_toks, vocabulary_list):\n",
    "\n",
    "#     num_docs = len(list(document_toks))\n",
    "#     raw_term_freqs = compute_raw_term_frequency(document_toks)\n",
    "#     term_wise_postings = generate_term_postings(document_toks)\n",
    "#     term_document_freq = compute_document_frequency(term_wise_postings)\n",
    "#     term_idfs = compute_IDF(term_document_freq, num_docs)\n",
    "#     num_words = len(vocabulary_list)\n",
    "#     tf_idf_by_scheme = {}\n",
    "#     schemes_list = ['binary', 'raw_count', 'term_frequency', 'log_normalization', 'double_normalization']\n",
    "#     for scheme in schemes_list:\n",
    "#         print(f\"Generating for scheme : {scheme}\")\n",
    "#         tf_idf_mat = pd.DataFrame(np.zeros((num_docs, num_words)), columns=vocabulary_list)\n",
    "#         for i in tqdm(range(num_docs)):\n",
    "#             # print(f\"for doc {i}\")\n",
    "#             for term in vocabulary_list:\n",
    "#                 tf_weight = compute_tf_weight(scheme, term, raw_term_freqs[i])\n",
    "#                 idf = term_idfs[term]\n",
    "#                 tf_idf_mat.iloc[i][term] = tf_weight * idf\n",
    "#         tf_idf_by_scheme[scheme] = tf_idf_mat\n",
    "#     return tf_idf_by_scheme\n",
    "\n",
    "def get_query_vector(query_toks, scheme, term_idfs, vocab_len):\n",
    "    num_query_toks = len(query_toks)\n",
    "    query_vector = [0] * vocab_len\n",
    "    query_tfs = {}\n",
    "    for i in range(num_query_toks):\n",
    "        query_tfs[query_toks[i]] = 0\n",
    "    for i in range(num_query_toks):\n",
    "        # term_id = term_to_id[query_toks[i]]\n",
    "        # query_vector[term_id] += 1\n",
    "        query_tfs[query_toks[i]] += 1\n",
    "    for i in range(len(query_vector)):\n",
    "        term_tf_weight = compute_tf_weight(scheme, id_to_term[i], query_tfs)\n",
    "        # print(id_to_term[i])\n",
    "        if(id_to_term[i] not in term_idfs.keys()):\n",
    "            idf_val = 0\n",
    "        else:\n",
    "            idf_val = term_idfs[id_to_term[i]]\n",
    "        # print(f\"idf_val = {idf_val}\")\n",
    "        query_vector[i] = term_tf_weight * idf_val\n",
    "    a, b = np.unique(query_vector, return_counts=True)\n",
    "    # print(f\"Query vector = {a} | {b}\")\n",
    "    return query_vector\n",
    "def process_tf_idf_query(query, tf_idf_matrix_dict, idf_values, vocab_len):\n",
    "    query_toks = cleanQuery(query)\n",
    "    print(f\"Query : {query}\")\n",
    "    print(f\"Query tokens = {query_toks}\\n\")\n",
    "    schemes_list = ['binary', 'raw_count', 'term_frequency', 'log_normalization', 'double_normalization']\n",
    "    # schemes_list = ['binary']\n",
    "    for scheme in schemes_list:\n",
    "        print(f\"\\n---------------- Scheme : {scheme} -------------------\\n\") \n",
    "        query_vector = np.array(get_query_vector(query_toks, scheme, idf_values, vocab_len)).reshape((vocab_len, 1)) # v x 1\n",
    "        tf_idf_matrix = np.array(tf_idf_matrix_dict[scheme]) #d x v\n",
    "        document_scores = np.dot(tf_idf_matrix, query_vector)\n",
    "        document_id_to_score = {}\n",
    "        for i in range(document_scores.shape[0]):\n",
    "            document_id_to_score[i] = document_scores[i][0]\n",
    "        document_id_to_score = dict(sorted(document_id_to_score.items(), key=lambda item: item[1], reverse=True))\n",
    "        top_5_doc_ids = list(document_id_to_score.keys())[:5]\n",
    "        top_5_doc_names = getDocsFromID(docID_to_doc_mapping, top_5_doc_ids)\n",
    "        for i in range(5):\n",
    "            print(f\"{i} -> {top_5_doc_names[i]} | Score = {document_id_to_score[top_5_doc_ids[i]]}\")\n",
    "        print(\"-----------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num docs = 1133\n",
      "Generating for scheme : binary\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1133/1133 [01:34<00:00, 12.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating for scheme : raw_count\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1133/1133 [01:27<00:00, 12.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating for scheme : term_frequency\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1133/1133 [01:51<00:00, 10.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating for scheme : log_normalization\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1133/1133 [01:27<00:00, 12.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating for scheme : double_normalization\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1133/1133 [01:34<00:00, 12.04it/s]\n"
     ]
    }
   ],
   "source": [
    "raw_tf_docs, term_idfs, tf_idf_matrix_dict  = generate_tf_idf_matrices(document_toks, vocabulary_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(tf_idf_matrix_dict, open('./tf_idf_matrices_q1.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_to_read = open(\"./tf_idf_matrices_q1.pkl\", \"rb\")\n",
    "tf_idf_matrix_dict = pickle.load(file_to_read)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1133, 82779)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(tf_idf_matrix_dict['binary']).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query : good\n",
      "Query tokens = ['good']\n",
      "\n",
      "\n",
      "---------------- Scheme : binary -------------------\n",
      "\n",
      "0 -> 1st_aid.txt | Score = 0.10409730823826448\n",
      "1 -> abbott.txt | Score = 0.10409730823826448\n",
      "2 -> acronyms.txt | Score = 0.10409730823826448\n",
      "3 -> ads.txt | Score = 0.10409730823826448\n",
      "4 -> adt_miam.txt | Score = 0.10409730823826448\n",
      "-----------------------------------------------------------\n",
      "\n",
      "---------------- Scheme : raw_count -------------------\n",
      "\n",
      "0 -> manners.txt | Score = 8.848271200252482\n",
      "1 -> practica.txt | Score = 5.621254644866283\n",
      "2 -> oldtime.sng | Score = 4.684378870721901\n",
      "3 -> mlverb.hum | Score = 4.059795021292315\n",
      "4 -> vegan.rcp | Score = 3.8516004048157857\n",
      "-----------------------------------------------------------\n",
      "\n",
      "---------------- Scheme : term_frequency -------------------\n",
      "\n",
      "0 -> oldtime.sng | Score = 0.007173627673387291\n",
      "1 -> f_tang.txt | Score = 0.004053346515472245\n",
      "2 -> popmach | Score = 0.0033579776851053057\n",
      "3 -> bless.bc | Score = 0.0023658479145060108\n",
      "4 -> beer.hum | Score = 0.0022986787308134113\n",
      "-----------------------------------------------------------\n",
      "\n",
      "---------------- Scheme : log_normalization -------------------\n",
      "\n",
      "0 -> manners.txt | Score = 0.06062024096050611\n",
      "1 -> practica.txt | Score = 0.05453672269832806\n",
      "2 -> oldtime.sng | Score = 0.052104864881494094\n",
      "3 -> mlverb.hum | Score = 0.05020281233363449\n",
      "4 -> vegan.rcp | Score = 0.04950475004557565\n",
      "-----------------------------------------------------------\n",
      "\n",
      "---------------- Scheme : double_normalization -------------------\n",
      "\n",
      "0 -> curse.txt | Score = 133740.40166561474\n",
      "1 -> a_tv_t-p.com | Score = 133715.67273826225\n",
      "2 -> strine.txt | Score = 133713.7630335622\n",
      "3 -> growth.txt | Score = 133703.88595301862\n",
      "4 -> turbo.hum | Score = 133703.70962464626\n",
      "-----------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "input_query = input()\n",
    "process_tf_idf_query(input_query, tf_idf_matrix_dict, term_idfs, len(vocabulary_list))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2370e07001db70a9c24f7e21173c51fbc4321340913a02830aed4885459fa0a0"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
