{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\Samyak\n",
      "[nltk_data]     Jain\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to C:\\Users\\Samyak\n",
      "[nltk_data]     Jain\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to C:\\Users\\Samyak\n",
      "[nltk_data]     Jain\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to C:\\Users\\Samyak\n",
      "[nltk_data]     Jain\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import copy\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = 'data/Humor,Hist,Media,Food'\n",
    "file_names = os.listdir(data_dir) #reading the data directory to list all the files\n",
    "file_paths = [(data_dir + '/' + fname) for fname in file_names] #forming file paths\n",
    "docID_to_doc_mapping = {} #forming docID to doc name mapping\n",
    "for i in range(len(file_names)):\n",
    "    docID_to_doc_mapping[i] = file_names[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punct(tok):\n",
    "    '''\n",
    "        Removing punctations from tokens\n",
    "    '''\n",
    "    punctuations = string.punctuation\n",
    "    tok = ''.join(x for x in tok if x not in punctuations)\n",
    "    return tok\n",
    "def remove_space(tok):\n",
    "    '''\n",
    "        Removing blank space toks\n",
    "    '''\n",
    "    tok = ''.join(x for x in tok if x != ' ')\n",
    "    return tok\n",
    "\n",
    "def preprocess_file(file_text):\n",
    "    '''\n",
    "        This function preprocesses the file text.\n",
    "        Input: file_text in string form represting the text of a file\n",
    "        Returns: cleaned_toks, word tokens present in the file after preprocessing\n",
    "    '''\n",
    "\n",
    "    #converting the text to lowercase\n",
    "    ftext = file_text.lower()\n",
    "\n",
    "    #performing word tokenization\n",
    "    file_toks = word_tokenize(ftext)\n",
    "\n",
    "    #removing the stopwords from tokens\n",
    "    stop_words = list(set(stopwords.words('english')))\n",
    "    file_toks = [tok for tok in file_toks if tok not in stop_words]\n",
    "\n",
    "    #removing punctuation marks from tokens\n",
    "    toks_no_punct = []\n",
    "    for tok in file_toks:\n",
    "        ctok = remove_punct(tok)\n",
    "        if(ctok != \"\"):\n",
    "            toks_no_punct.append(ctok)\n",
    "    \n",
    "    #Removing blank space tokens\n",
    "    cleaned_toks = []\n",
    "    for tok in toks_no_punct:\n",
    "        ctok = remove_space(tok)\n",
    "        if(ctok != \"\"):\n",
    "            cleaned_toks.append(ctok)\n",
    "\n",
    "    return cleaned_toks\n",
    "\n",
    "def cleanQuery(query_text):\n",
    "    '''\n",
    "        Preprocessing the query text\n",
    "        Input: query_text, string of the phrase query text\n",
    "        Returns: cleaned_toks, an array containg the preprocessed query tokens\n",
    "    '''\n",
    "\n",
    "    #We perform the same preprocessing steps on the query as we did for the file text\n",
    "\n",
    "    #converting the text to lowercase\n",
    "    qtext = query_text.lower()\n",
    "    \n",
    "    #performing word tokenization\n",
    "    query_toks = word_tokenize(qtext)\n",
    "    \n",
    "    #removing the stopwords from tokens\n",
    "    stop_words = list(set(stopwords.words('english')))\n",
    "    query_toks = [tok for tok in query_toks if tok not in stop_words]\n",
    "    \n",
    "    #removing punctuation marks from tokens\n",
    "    toks_no_punct = []\n",
    "    for tok in query_toks:\n",
    "        ctok = remove_punct(tok)\n",
    "        if(ctok != \"\"):\n",
    "            toks_no_punct.append(ctok)\n",
    "    \n",
    "    #Removing blank space tokens\n",
    "    cleaned_toks = []\n",
    "    for tok in toks_no_punct:\n",
    "        ctok = remove_space(tok)\n",
    "        if(ctok != \"\"):\n",
    "            cleaned_toks.append(ctok)\n",
    "    \n",
    "    return cleaned_toks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(fpaths):\n",
    "    '''\n",
    "        Reads the files and preprocess every file's text to form word tokens for every file.\n",
    "        Returns a 2-D list containing word tokens for every file\n",
    "    '''\n",
    "    file_tokens = []\n",
    "    for fpath in fpaths:\n",
    "        f = open(fpath, 'r', encoding='utf-8', errors='ignore') #open the file\n",
    "        ftxt_unprocessed = f.read() #read the text of the file\n",
    "        ftoks = preprocess_file(ftxt_unprocessed) #preprocessing the text to form word tokens\n",
    "        file_tokens.append(ftoks)\n",
    "    return file_tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDocsFromID(docID_to_doc, doc_IDs):\n",
    "    '''\n",
    "        Given a list of document IDs, it outputs the document names corresponding to thos IDs.\n",
    "        Input: docID_to_docs (mapping between docID -> doc_name), docIDs - list of input document IDs\n",
    "        Returns: doc_names - list of doc_names corresponding to document IDs in doc_IDs\n",
    "    '''\n",
    "    doc_names = []\n",
    "    for doc_ID in doc_IDs:\n",
    "        doc_names.append(docID_to_doc[doc_ID])\n",
    "    return doc_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_toks = read_file(file_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary_set = set()\n",
    "for doc_tok in document_toks:\n",
    "    for tok in doc_tok:\n",
    "        vocabulary_set.add(tok)\n",
    "vocabulary_list = list(vocabulary_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_jaccard_coeff(query_toks, doc_toks):\n",
    "    query_tok_set = set(query_toks)\n",
    "    doc_toks_set = set(doc_toks)\n",
    "    num_intersection = len(list(query_tok_set & doc_toks_set))\n",
    "    num_union = len(list(query_tok_set | doc_toks_set))\n",
    "    jaccard = num_intersection / num_union\n",
    "    return jaccard\n",
    "\n",
    "def perform_jaccard_scoring(query_toks, document_toks):\n",
    "    jaccard_coeff_values = {}\n",
    "    for i in range(len(document_toks)):\n",
    "        jaccard_coeff_i = compute_jaccard_coeff(query_toks, document_toks[i])\n",
    "        jaccard_coeff_values[i] = jaccard_coeff_i\n",
    "    return jaccard_coeff_values\n",
    "\n",
    "def get_relevant_by_jaccard(jaccard_coeff_values):\n",
    "    ranked_order_by_jaccard = dict(sorted(jaccard_coeff_values.items(), key=lambda item: item[1], reverse=True))\n",
    "    top_5_docID = list(ranked_order_by_jaccard.keys())[:5]\n",
    "    top_5_jaccard = list(ranked_order_by_jaccard.values())[:5]\n",
    "    return top_5_docID, top_5_jaccard\n",
    "\n",
    "def run_jaccard(query, document_toks):\n",
    "    query_toks = cleanQuery(query)\n",
    "    if(len(query_toks) == 0):\n",
    "        print(\"no. of query tokens after preprocessing is 0. Jaccard coefficient with all documents is equal to 0\")\n",
    "        for i in range(5):\n",
    "            print(f\"{i + 1} : {docID_to_doc_mapping[i]} (0)\") \n",
    "    jaccard_scores = perform_jaccard_scoring(query_toks, document_toks)\n",
    "    print(jaccard_scores)\n",
    "    top_5_doc_ids, top_5_jaccard_score = get_relevant_by_jaccard(jaccard_scores)\n",
    "    top_5_doc_names = getDocsFromID(docID_to_doc_mapping, top_5_doc_ids)\n",
    "    print(f\"Query Text = {query}\")\n",
    "    print(f\"Query tokens after preprocessing = {query_toks}\")\n",
    "    print(f\"Top 5 relevant documents based on the value of the Jaccard coefficient : \")\n",
    "    for i in range(len(top_5_doc_names)):\n",
    "        print(f\"{i + 1} : {top_5_doc_names[i]} ({top_5_jaccard_score[i]})\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_raw_term_frequency(document_toks):\n",
    "\n",
    "    raw_term_freq = {}\n",
    "    for i in range(len(document_toks)):\n",
    "        raw_term_freq[i] = {}\n",
    "        unique_toks, tok_freq = np.unique(document_toks[i], return_counts=True)\n",
    "        for j in range(len(unique_toks)):\n",
    "            raw_term_freq[i][unique_toks[j]] = tok_freq[j]\n",
    "    return raw_term_freq\n",
    "        # for term in document_toks[i]:\n",
    "        #     if(term in raw_term_freq[i].keys()):\n",
    "        #         raw_term_freq[i][term] += 1\n",
    "        #     else:\n",
    "        #         raw_term_freq[i][term] = 1\n",
    "def generate_term_postings(document_toks):\n",
    "\n",
    "    term_posting_lists = {}\n",
    "    for i in range(len(document_toks)):\n",
    "        for tok in document_toks[i]:\n",
    "            if(tok not in term_posting_lists.keys()):\n",
    "                term_posting_lists[tok] = [i]\n",
    "            else:\n",
    "                if(i not in term_posting_lists[tok]):\n",
    "                    term_posting_lists[tok].append(i)\n",
    "    return term_posting_lists\n",
    "\n",
    "def compute_document_frequency(term_posting_lists):\n",
    "\n",
    "    term_df = {}\n",
    "    for term in term_posting_lists.keys():\n",
    "        term_df[term] = len(term_posting_lists[term])\n",
    "    return term_df\n",
    "\n",
    "def compute_IDF(term_df, num_total_docs):\n",
    "\n",
    "    term_idf = {}\n",
    "    for term in term_df.keys():\n",
    "        idf_value = np.log10(num_total_docs / (term_df[term] + 1))\n",
    "        term_idf[term] = idf_value\n",
    "    return term_idf\n",
    "\n",
    "def compute_tf_weight(scheme, term, doc_tfs):\n",
    "\n",
    "    if(scheme == \"binary\"):\n",
    "        if(term in doc_tfs.keys()):\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "    elif(scheme == \"raw_count\"):\n",
    "        if(term in doc_tfs.keys()):\n",
    "            return doc_tfs[term]\n",
    "        else:\n",
    "            return 0\n",
    "    elif(scheme == \"term_frequency\"):\n",
    "        if(term in doc_tfs.keys()):\n",
    "            total_terms = sum(doc_tfs.values())\n",
    "            return doc_tfs[term] / total_terms\n",
    "        else:\n",
    "            return 0\n",
    "    elif(scheme == \"log_normalization\"):\n",
    "        if(term in doc_tfs.keys()):\n",
    "            return np.log10(1 + doc_tfs[term])\n",
    "        else:\n",
    "            return 0\n",
    "    elif(scheme == \"double_normalization\"):\n",
    "        if(term in doc_tfs.keys()):\n",
    "            t1 = 0.5\n",
    "            t2 = (0.5)*(doc_tfs[term] / max(doc_tfs.values()))\n",
    "            return t1 + t2\n",
    "        else:\n",
    "            return 0.5\n",
    "\n",
    "def generate_tf_idf_matrices(document_toks, vocabulary_list):\n",
    "\n",
    "    num_docs = len(list(document_toks))\n",
    "    raw_term_freqs = compute_raw_term_frequency(document_toks)\n",
    "    term_wise_postings = generate_term_postings(document_toks)\n",
    "    term_document_freq = compute_document_frequency(term_wise_postings)\n",
    "    term_idfs = compute_IDF(term_document_freq, num_docs)\n",
    "    num_words = len(vocabulary_list)\n",
    "    tf_idf_by_scheme = {}\n",
    "    schemes_list = ['binary', 'raw_count', 'term_frequency', 'log_normalization', 'double_normalization']\n",
    "    for scheme in schemes_list:\n",
    "        print(f\"Generating for scheme : {scheme}\")\n",
    "        tf_idf_mat = pd.DataFrame(np.zeros((num_docs, num_words)), columns=vocabulary_list)\n",
    "        for i in tqdm(range(num_docs)):\n",
    "            # print(f\"for doc {i}\")\n",
    "            for term in vocabulary_list:\n",
    "                tf_weight = compute_tf_weight(scheme, term, raw_term_freqs[i])\n",
    "                idf = term_idfs[term]\n",
    "                tf_idf_mat.iloc[i][term] = tf_weight * idf\n",
    "        tf_idf_by_scheme[scheme] = tf_idf_mat\n",
    "    return tf_idf_by_scheme\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating for scheme : binary\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1133/1133 [1:33:53<00:00,  4.97s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating for scheme : raw_count\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1133/1133 [1:16:28<00:00,  4.05s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating for scheme : term_frequency\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1133/1133 [1:31:42<00:00,  4.86s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating for scheme : log_normalization\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1133/1133 [1:18:30<00:00,  4.16s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating for scheme : double_normalization\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1133/1133 [1:13:18<00:00,  3.88s/it]\n"
     ]
    }
   ],
   "source": [
    "tf_idf_mat_by_scheme = generate_tf_idf_matrices(document_toks, vocabulary_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf_file = open(\"tf_idf_matrices\", \"wb\")\n",
    "pickle.dump(tf_idf_mat_by_scheme, tf_idf_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>unrecycled</th>\n",
       "      <th>gripology</th>\n",
       "      <th>sysadmin</th>\n",
       "      <th>perchlorate</th>\n",
       "      <th>waterfountain</th>\n",
       "      <th>fungus</th>\n",
       "      <th>samwich</th>\n",
       "      <th>41364</th>\n",
       "      <th>meatandtwoveg</th>\n",
       "      <th>bosix</th>\n",
       "      <th>...</th>\n",
       "      <th>barnum</th>\n",
       "      <th>recalling</th>\n",
       "      <th>012287</th>\n",
       "      <th>superbabs</th>\n",
       "      <th>devoisters</th>\n",
       "      <th>resister</th>\n",
       "      <th>wpa</th>\n",
       "      <th>18914</th>\n",
       "      <th>44inch</th>\n",
       "      <th>freeware</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.7532</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 82779 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   unrecycled  gripology  sysadmin  perchlorate  waterfountain  fungus  \\\n",
       "0         0.0        0.0       0.0          0.0            0.0     0.0   \n",
       "1         0.0        0.0       0.0          0.0            0.0     0.0   \n",
       "2         0.0        0.0       0.0          0.0            0.0     0.0   \n",
       "3         0.0        0.0       0.0          0.0            0.0     0.0   \n",
       "4         0.0        0.0       0.0          0.0            0.0     0.0   \n",
       "5         0.0        0.0       0.0          0.0            0.0     0.0   \n",
       "6         0.0        0.0       0.0          0.0            0.0     0.0   \n",
       "7         0.0        0.0       0.0          0.0            0.0     0.0   \n",
       "8         0.0        0.0       0.0          0.0            0.0     0.0   \n",
       "9         0.0        0.0       0.0          0.0            0.0     0.0   \n",
       "\n",
       "   samwich  41364  meatandtwoveg   bosix  ...  barnum  recalling  012287  \\\n",
       "0      0.0    0.0            0.0  0.0000  ...     0.0        0.0     0.0   \n",
       "1      0.0    0.0            0.0  0.0000  ...     0.0        0.0     0.0   \n",
       "2      0.0    0.0            0.0  0.0000  ...     0.0        0.0     0.0   \n",
       "3      0.0    0.0            0.0  0.0000  ...     0.0        0.0     0.0   \n",
       "4      0.0    0.0            0.0  0.0000  ...     0.0        0.0     0.0   \n",
       "5      0.0    0.0            0.0  0.0000  ...     0.0        0.0     0.0   \n",
       "6      0.0    0.0            0.0  0.0000  ...     0.0        0.0     0.0   \n",
       "7      0.0    0.0            0.0  0.0000  ...     0.0        0.0     0.0   \n",
       "8      0.0    0.0            0.0  0.0000  ...     0.0        0.0     0.0   \n",
       "9      0.0    0.0            0.0  2.7532  ...     0.0        0.0     0.0   \n",
       "\n",
       "   superbabs  devoisters  resister  wpa  18914  44inch  freeware  \n",
       "0        0.0         0.0       0.0  0.0    0.0     0.0       0.0  \n",
       "1        0.0         0.0       0.0  0.0    0.0     0.0       0.0  \n",
       "2        0.0         0.0       0.0  0.0    0.0     0.0       0.0  \n",
       "3        0.0         0.0       0.0  0.0    0.0     0.0       0.0  \n",
       "4        0.0         0.0       0.0  0.0    0.0     0.0       0.0  \n",
       "5        0.0         0.0       0.0  0.0    0.0     0.0       0.0  \n",
       "6        0.0         0.0       0.0  0.0    0.0     0.0       0.0  \n",
       "7        0.0         0.0       0.0  0.0    0.0     0.0       0.0  \n",
       "8        0.0         0.0       0.0  0.0    0.0     0.0       0.0  \n",
       "9        0.0         0.0       0.0  0.0    0.0     0.0       0.0  \n",
       "\n",
       "[10 rows x 82779 columns]"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_idf_mat_by_scheme['raw_count'].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def perform_ranking_by_tf_idf(tf_idf_mat_by_scheme, query):\n",
    "#     schemes_list = ['binary', 'raw_count', 'term_frequency', 'log_normalization', 'double_normalization']\n",
    "#     query_toks = cleanQuery(query)\n",
    "#     for scheme in schemes_list:\n",
    "#         score_with_doc = {}\n",
    "#         for i in range(len(document_toks)):\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating for scheme : raw_count\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1133/1133 [1:19:06<00:00,  4.19s/it]\n"
     ]
    }
   ],
   "source": [
    "num_docs = len(list(document_toks))\n",
    "raw_term_freqs = compute_raw_term_frequency(document_toks)\n",
    "term_wise_postings = generate_term_postings(document_toks)\n",
    "term_document_freq = compute_document_frequency(term_wise_postings)\n",
    "term_idfs = compute_IDF(term_document_freq, num_docs)\n",
    "num_words = len(vocabulary_list)\n",
    "# tf_idf_by_scheme = {}\n",
    "schemes_list = ['raw_count']\n",
    "for scheme in schemes_list:\n",
    "    print(f\"Generating for scheme : {scheme}\")\n",
    "    tf_idf_mat = pd.DataFrame(np.zeros((num_docs, num_words)), columns=vocabulary_list)\n",
    "    for i in tqdm(range(num_docs)):\n",
    "        # print(f\"for doc {i}\")\n",
    "        for term in vocabulary_list:\n",
    "            tf_weight = compute_tf_weight(scheme, term, raw_term_freqs[i])\n",
    "            idf = term_idfs[term]\n",
    "            tf_idf_mat.iloc[i][term] = tf_weight * idf\n",
    "    tf_idf_mat_by_scheme[scheme] = tf_idf_mat\n",
    "# return tf_idf_by_scheme"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2370e07001db70a9c24f7e21173c51fbc4321340913a02830aed4885459fa0a0"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
